% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt]{article}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% http://www.sciencemag.org/authors/preparing-manuscripts-using-latex 
% This package should properly format in-text
% reference calls and reference-list numbers.

\usepackage{adjustbox}
\usepackage{scicite}

\usepackage{times}
\usepackage{units}

%\usepackage[T1]{fontenc}
%\usepackage[ngerman]{babel}
\usepackage[english]{babel}
\usepackage{empheq}

\usepackage[]{graphicx}
\graphicspath{ {./fig/} }
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\usepackage[labelformat=simple]{subcaption}  
\captionsetup[subfigure]{font={bf,small}, skip=1pt, margin=-0.1cm, singlelinecheck=false}
\usepackage[labelfont=bf]{caption}
\captionsetup{labelfont=bf}
\captionsetup{font=footnotesize}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{dirtytalk}
%\usepackage{fourier}
\usepackage{siunitx}
\usepackage{tcolorbox}
\usepackage{wrapfig}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

% Added by authors
\usepackage{siunitx}
\usepackage{tabularx,ragged2e,booktabs}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{bm}
\usepackage{mathtools}
\captionsetup[figure]{name={Fig.},labelsep=period}
%\captionsetup[table]{name={Table},labelsep=period}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{xr}
\externaldocument{supplementary_materials}


%\usepackage[demo]{graphicx}
%\usepackage{ifdraft}
%\ifdraft{\renewcommand{\includegraphics}{\relax}}{\relax}
%\usepackage{comment}
%\excludecomment{figure}
%\let\endfigure\relax


\newcommand\hl[1]{\colorbox{yellow}{\textcolor{red}{#1}}}
\newcommand\myhl[1]{\textcolor{red}{#1}}



% Use this to display line numnbers
%\usepackage{lineno}
%\linenumbers

\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\renewcommand{\emph}[1]{\textit{#1}}
\let\textcircledold\textcircled

\renewcommand{\textcircled}[1]{\raisebox{.5pt}{\textcircledold{\raisebox{-.45pt} {#1}}}}
\newcommand{\pigraph}{$\pi$-graph}
\newcommand*{\important}[1]{\textcolor{red}{\danger~\textbf{IMPORTANT:~}} \textcolor{red}{#1}}
\newcommand*{\pending}[1]{\textcolor{blue}{$\bigstar$~\textbf{PENDING~#1}}}
\newcommand\mybox[2][]{\tikz[overlay]\node[fill=blue!100,inner sep=4pt, anchor=text, rectangle, rounded corners=1mm,#1] {#2};\phantom{#2}}

\newcommand{\TODO}[1]{\mybox[fill=yellow]{\textcolor{blue}{\Large \textbf{TODO}}:~\textcolor{blue}{\textbf{\emph{#1}}}}}
\newcommand{\xmark}{\ding{55}}%
\newcommand{\textcircledD}[1]{\raisebox{.9pt}{\textcircled{\raisebox{+.5pt} {\footnotesize#1}}}}
\newcommand{\hu}[1]{\textcolor{orange}{[Hu: #1]}}
\newcommand{\kuehn}[1]{\textcolor{blue}{[Kuehn: #1]}}
\newcommand{\diaz}[1]{\textcolor{blue}{[Diaz: #1]}}
\newcommand{\haddadin}[1]{\textcolor{red}{[Haddadin: #1]}}
\newcommand{\del}[1]{\textcolor{orange}{\xout{#1}}}
\newcommand{\new}[1]{\textcolor{orange}{#1}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\renewcommand{\thesubfigure}{\textbf{\Alph{subfigure}}}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

\renewcommand{\figurename}{Fig.}


%% MY ADDED SECTION
\usetikzlibrary{backgrounds}
\makeatletter

\tikzset{%
	fancy quotes/.style={
		text width=\fq@width pt,
		align=justify,
		inner sep=1em,
		anchor=north west,
		minimum width=\linewidth,
	},
	fancy quotes width/.initial={.8\linewidth},
	fancy quotes marks/.style={
		scale=8,
		text=white,
		inner sep=0pt,
	},
	fancy quotes opening/.style={
		fancy quotes marks,
	},
	fancy quotes closing/.style={
		fancy quotes marks,
	},
	fancy quotes background/.style={
		show background rectangle,
		inner frame xsep=0pt,
		background rectangle/.style={
			fill=gray!25,
			rounded corners,
		},
	}
}

\newenvironment{fancyquotes}[1][]{%
	\noindent
	\tikzpicture[fancy quotes background]
	\node[fancy quotes opening,anchor=north west] (fq@ul) at (0,0) {``};
	\tikz@scan@one@point\pgfutil@firstofone(fq@ul.east)
	\pgfmathsetmacro{\fq@width}{\linewidth - 2*\pgf@x}
	\node[fancy quotes,#1] (fq@txt) at (fq@ul.north west) \bgroup
}
{\egroup;
	\node[overlay,fancy quotes closing,anchor=east] at (fq@txt.south east) {''};
	\endtikzpicture}

\makeatother
\newcommand{\task}{\ensuremath{\tau}}
\newcommand{\sltwoi}{\ensuremath{t_l}} %single learning time without index
\newcommand{\slt}[1]{\ensuremath{t_{l,#1}}} %... with index
\newcommand{\tlt}{\ensuremath{T}} %total learning time
\newcommand{\comp}{\ensuremath{c}} %complexity (learning time from scratch)
\newcommand{\diste}[1]{\ensuremath{\mathrm{d}(\task_{#1},\{ \})}}
\newcommand{\dist}[2]{\ensuremath{\mathrm{d}(\task_{#1},\{\task_1, \task_2, \dots, \task_{#2}\})}}
\newcommand{\En}{\ensuremath{E}}
\newcommand{\opt}{\ensuremath{\mathrm{opt}}}
\newcommand{\tot}{\ensuremath{\mathrm{tot}}}
\newcommand{\Opt}{\ensuremath{\mathrm{Opt}}}
\newcommand{\densMan}{\ensuremath{\rho_{\mathrm{man}}}} %manufacturing energy density
\newcommand{\Tau}{\ensuremath{\mathcal{T}}}

\setlength{\columnsep}{1cm}

\newtheorem{challenge}{\textbf{CHALLENGE}}

\renewcommand{\arraystretch}{2} 

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.
\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm


%The next command sets up an environment for the abstract to your paper.
\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}


% Include your paper's title here
\title{Collective knowledge sharing allows minimum learning time and sustainable energy balance in embodied AI}

% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.

\author
{Fernando D\'iaz Ledezma$^{\ast}$ and Sami Haddadin
\\
\normalsize{Chair of Robotics and Systems Intelligence,}\\
\normalsize{MIRMI - Munich Institute of Robotics and Machine Intelligence,}\\
\normalsize{Technical University of Munich, Georg-Brauchle-Ring 60-62, M\"unchen, 80992, Germany}\\
\\
\normalsize{$^\ast$To whom correspondence should be addressed; E-mail: fernando.diaz@tum.de}
}

% Include the date command, but leave its argument blank.

\date{}



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document} 
% Double-space the manuscript.

\baselineskip24pt

% Make the title.

\maketitle 



% Place your abstract within the special {sciabstract} environment.
\begin{sciabstract}
	Classical artificial intelligence (AI) and its current learning paradigms consume significant amounts of energy due to high computational loads and limited utilization of acquired knowledge. As AI and robotics merge to form embodied AI systems, their energy demand will continue to rise as data acquisition and learning rely on constant interaction with the physical environment. In this study, we examine the fundamental energy requirements of embodied AI systems and discuss the escalating energy challenges associated with maintaining current learning paradigms. Consequently, we propose collective learning as a paradigm shift that departs from isolated learning and expands on the concepts of incremental and transfer learning. Collective learning enables efficient learning in embodied AI agents by actively sharing, aggregating, and utilizing previous and current knowledge across systems to acquire new skills in shorter timeframes, significantly reducing energy consumption.
\end{sciabstract}

% In setting up this template for *Science* papers, we've used both
% the \section* command and the \paragraph* command for topical
% divisions.  Which you use will of course depend on the type of paper
% you're writing.  Review Articles tend to have displayed headings, for
% which \section* is more appropriate; Research Articles, when they have
% formal topical divisions at all, tend to signal them with bold text
% that runs into the paragraph, for which \paragraph* is the right
% choice.  Either way, use the asterisk (*) modifier, as shown, to
% suppress numbering.

%%%%%% Main Text %%%%%%

\newcommand{\beginsupplement}
{%
	\setcounter{table}{0}
	\renewcommand{\thesection}{S\arabic{section}}
	\renewcommand{\thetable}{S\arabic{table}}%
	\setcounter{figure}{0}
	\renewcommand{\thefigure}{S\arabic{figure}}%
}


% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Introduction}\label{sec:intro}
As AI technology develops and permeates many aspects of human life, not only will smart factories become the norm \cite{Szczepanski2019Economicimpactsartificial}, but healthcare services will modernize to integrate the analysis and prediction power of AI services. Households will morph into mostly automated environments. The future will witness the widespread presence of modern robots in various sectors such as industry, logistics, service, and healthcare. These robots will possess local and network computing capabilities, enabling them to operate in these environments and gather and exchange information. Artificial intelligence (AI) will be an inherent feature of these robots, empowering them to acquire new skills and disseminate their acquired knowledge across different systems. The more these \emph{embodied AI} (EIA) agents integrate synergistically into varied environments, the more they will take over diverse tasks while also actively cooperating with humans. Yet, among the many challenges emerging as EAI agents become ubiquitous, their energy demand deserves particular attention.

% SUBSECTION ========================================================================================
Classical AI (CAI) interprets intelligence as a purely computational symbol-processing problem decoupled from physical agents and interaction with the world. Progress in CAI relies on two computing stages: (i) learning and (ii) deployment. While the latter is not energy-efficient compared to its biological counterparts, the former craves a considerable energy quota to process large amounts of data for training, validation, and testing of models (e.g., supercomputing). Furthermore, both model and training data are expected to carry enough information to transfer knowledge between different tasks and even systems (at least to a certain extent). However, retraining is required when this information is unavailable in the data ---sometimes even from scratch---, leading to highly energy-inefficient learning paradigms. The implication is that the energetic cost of CAI can sometimes outweigh the benefits \cite{Strubell2019EnergyPolicyConsiderations}. Even current breakthroughs like the use of transformer models for Natural Language Processing exhibit energetic challenges \cite{Cao2020TowardsAccurateReliable}.
% ---
\begin{figure}[!t]
	\centering
	\includegraphics[width=0.95\textwidth]{fig/embodied_ai_learning_pipeline_v7.png}
	\caption{Standard skill execution pipeline of isolated EAI agents.}
	\label{fig:embodied_ai_pipeline}
\end{figure}
% ---

With the evolution towards EAI systems, i.e., the integration of AI and robotics, the challenges mentioned before expand. Since the real world cannot be faithfully replicated in virtual environments, and despite the considerable advances in sim-to-real applications \cite{Chebotar2019Closingsimreal}, data acquisition for EAI agents necessitates constant and active energy-expending interaction with the physical environment. Learning a skill in the real world implies physically executing it several times, spending energy on motion and interaction during each execution. Consider, for example, autonomous driving, where the vehicle is a rather rudimentary form of an EAI agent. Despite operating primarily in a structured human-made environment, complexity is already present in the system. The vehicle consumes energy by simply fulfilling its purpose of autonomous movement and collecting vast amounts of data during driving to retrain and improve the policy model. Another, perhaps inconspicuous, example is household robots. Current estimates indicate that 39~\%  of domestic chores could be automated in the short term \cite{Lehdonvirta2022futuresunpaidwork}. Even though, superficially, homes may be less complex when compared to factory floors, they are extremely challenging dynamic environments that require household robots to undergo constant retraining. 

When an EAI agent executes a skill, a lower bound exists to the energy requirements. Consider a generic skill $\tau$ ---such as a pick-and-place operation--- and suppose the optimal trajectory $p^\star$ for moving an object from its origin to its destination is known. The intrinsic properties of the object and the optimal path $p^\star$ uniquely define the minimum energy requirement $E^\star_{\tau}$ needed to perform skill $\tau$. In practice, however, the total energy expended in mastering and executing a skill encompasses not only $E^\star_{\tau}$. The standard skill execution pipeline of a stereotypical EAI agent (Fig.~\ref{fig:embodied_ai_pipeline}) allows the identification of three fundamental energetic expenditure categories, namely:
% ---
\begin{enumerate}
	\item Computation and Communication Expenditure (CCE): Coincident with CAI, it refers to the energy used by the computation and communication processes required by planning, querying, exploration, and training routines.
	\item Basal Energy Expenditure (BEE): This body-related energy is associated with the execution of basic functions of the EAI agent. For example, operating energy, gravity compensation, and proprioceptive intelligence algorithms in robots, hovering in drones, running on-board system standby in autonomous vehicles, etc.
	\item Motion and Interaction Expenditure (MIE): Defines the energy spent on physical interactions, i.e., executing a particular skill in a certain form. For example, taking an object from an initial to a target location within a given time following a particular trajectory.
\end{enumerate}
% ---
Consequently, any agent aiming to accomplish $\tau$ will unavoidably expend at least $E^*_{\tau}$ plus the contributions of the computational energy ($E_{CCE}$), the body-related energy ($E_{BEE}$), and the physical interaction energy ($E_{MIE}$) throughout the learning and deployment phases; i.e.
% ---
\begin{equation}
	E_{\tau} =  \underbrace{E^\star_{\tau}}_{\text{Skill energy}} + \overbrace{E_{BEE}}^{\text{Body-dependent energy}} + \underbrace{E_{CCE} + E_{MIE}}_{\text{Learning energy}}.
\end{equation}
% ---

While the energetic requirements of EAI have only recently gained attention within the AI and robotics research communities, grasping its full significance necessitates recognizing the challenges posed by the substantial escalation of the CCE, BEE, and MIE energy expenditures associated with the rapid increase in the population of EAI agents. Of vital importance is underscoring the negative impact of persistently employing learning paradigms that do not actively foster and systematically leverage knowledge exchange and learning transfer among agents.

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Energy grand challenges in EAI}\label{sec:energy_grand_challenges}
% ---
\begin{figure*}[!t]
	\centering
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{data_center_energy_consumption.png} \label{fig:dataCenterEnergy}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{ir_energy_projections.png} \label{fig:ir_energy}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{cb_energy_projections.png} \label{fig:cobot_energy}
	\end{subfigure}	
	\hspace*{\fill}
	\caption[] {\label{fig:energy_demands_AI_robotics} \textbf{Energy demands in AI and robotics.} \subref{fig:dataCenterEnergy} Global electricity demand of data centers, adapted from \cite{andrae2015global}. The estimated World Robot Energy Consumption of \subref{fig:ir_energy} industrial robots and \subref{fig:cobot_energy} cobots.}
\end{figure*}
% ---

% SUBSECTION ========================================================================================
\paragraph*{\textbf{Challenge 1} (C1): Energy for AI infrastructure}
The remarkable progress across various domains, attributed to the exponential growth of AI and machine learning applications, comes with costs. These advancements demand substantial computational power for cutting-edge machine learning algorithms to process, analyze, and learn from extensive data, often requiring numerous iterations to converge \cite{Strubell2019EnergyPolicyConsiderations}. Researchers and corporations rely on existing infrastructure or cloud computing services in data centers for energy-intensive model training and deployment. This has increased energy consumption, driven by demanding computational workloads in data centers and associated hardware like Graphics Processing Units (GPUs). For instance, in Germany, data center energy consumption surged from 10 TWh in 2010 to 17 TWh in 2021, with projections reaching 28 TWh by 2030 \cite{Hintemann2022Cloudcomputingdrives}. Globally, data center energy consumption rose from 200 TWh to an estimated 220-320 TWh between 2015 and 2021\footnote{Data from the International Energy Agency, available at \url{https://www.iea.org/reports/data-centres-and-data-transmission-networks}}. This trend is illustrated in Figure~\ref{fig:dataCenterEnergy}.

This escalating energy usage has sparked growing concerns within the research community regarding the adverse environmental effects of AI and machine learning. Recent discussions on this matter, such as \cite{schwartz2019green}, \cite{vinuesa2020role}, and \cite{Strubell2019EnergyPolicyConsiderations}, delve into the efficiency of computation-intensive deep learning algorithms. Notably, the staggering increase---more than 300,000 times over the last decade---in the number of computations required by these algorithms is alarming \cite{schwartz2019green}. Various metrics have been established to gauge the energy consumption of machine learning algorithms. These include assessing energy efficiency during development phases \cite{zhou2020hulk}, analyzing accuracy, model size, time, and CPU/GPU energy consumption for training and inference phases \cite{Dalgren2019GreenMLA}, as well as encompassing other system-level performance indicators like real-time metrics, instruction-level analysis, and hardware-level power estimation \cite{garcia2019estimation}. Despite the burgeoning awareness surrounding AI's energy consumption, tangible actions remain scarce in addressing the underlying issues and proposing potential remedies.

Complicating matters, unlike state-of-the-art machine learning models (e.g., transformer models) that are mostly trained once on a large amount of data, EAI systems will use models that require constant retraining and re-evaluation, such as neural architecture search \cite{real2019regularized}. Another key contributor to the energy consumption of EAI systems emerged due to recent techniques that increased data sample complexity from precisely located sensor measurements to full camera images, scaling up the number of computations required \cite{krizhevsky2012imagenet}.

% SUBSECTION ========================================================================================
\paragraph*{\textbf{Challenge 2} (C2): The Escalating Energy Demand of a Robotic Revolution}\label{sec:robots_challenge}
The continuous growth in the number of robots in operation is a notable trend amplified by the rise of Industry 4.0 and the implementation of smart factories, alongside the expanding utilization of robots in various service-oriented applications. Despite the advancements in robot technology that have yielded improved energy efficiency, the predominant focus remains on individual systems, often disregarding the collective impact of all active units.

Over the past decade, the installation base of industrial robots has undergone a remarkable transformation, escalating from 1.2 million units in 2012 to approximately 3.5 million units in 2022, an astonishing surge constituting a 290 \% increase. According to data from the International Federation of Robotics (IFR), the annual growth rate within this time frame has consistently ranged between 12 \% and 15 \% \cite{IFR2019}. Extrapolation of this growth rate suggests that in the coming years, four million robots will be operational within factories across the globe. This trend is depicted in Fig.\ref{fig:ir_stock}\footnote{These projections closely align with the slightly more cautious estimates presented by \textit{The Boston Consulting Group} in \cite{sirkin2015}.}. Using the estimated install base and under the assumption of round-the-clock operation, we can approximate the forthcoming energy demand attributable to industrial robots---termed the \textit{World Robot Energy Consumption} (WREC), shown in Fig.~\ref{fig:ir_energy}. A description of how we arrived at these estimates is provided in the \nameref{sec:supplementary_materials} Sec.~\ref{sec:app_robot_ener_consumption}. To contextualize the WREC, in 2025 it constitutes 7.2 \% of Germany's installed electricity generation capacity \cite{fraunhofer2016}.

The far-reaching influence of collaborative and service robots echoes the significance observed among their industrial counterparts. Collaborative robots (cobots), for instance, have undergone a paradigm shift, progressing from accounting for a mere 6 \% of the market in 2017 to constituting around one-quarter of annual installations \cite{tobe2015}, as illustrated in Fig.~\ref{fig:industrial_cobot_share}. Drawing from analogous assumptions applied to industrial robots (see Sec.~\ref{sec:app_cobot_ener_consumption}), Fig.~\ref{fig:cobot_stock} and \ref{fig:cobot_energy} lay out the projected growth trajectory and the corresponding energy consumption within this robot category. Concurrently, the domain of service robots is experiencing an analogous surge. For instance, the International Federation of Robotics estimated sales of approximately 35 million privately used service robots in 2018 \cite{IFR2015}. These robots find utility across various fields, including logistics, defense, public relations, medical applications, and beyond, underlining their alignment with the escalating trends observed among industrial and collaborative robots.

% SUBSECTION ========================================================================================
\paragraph*{\textbf{Challenge 3} (C3): Energy for manufacturing}
This final challenge broadens our perspective, encompassing the energetic expenditure associated with manufacturing the EAI agents. While not our primary focus, we touch upon this aspect for comprehensiveness and to encourage its exploration in forthcoming research. C3 entails two primary facets. First, it involves the energy outlay for procuring the materials for robot manufacturing and the associated computational hardware. Second, it pertains to the energy consumption intrinsic to the manufacturing process. Given the direct correlation between energy demand and the quantity of EAI agents produced, an exponential rise in their numbers directly corresponds to escalated energy consumption for their production. The assessment and formulation of strategies to address this aspect constitute the crux of this challenge. While an immediate solution may not be evident, and since substantial energy savings in raw material procurement may be impractical, significant potential lies in the domain of robot recycling strategies as a means of conserving energy \footnote{An example of such an endeavor is the international competition  \textit{Robothon\textsuperscript{\textregistered} - The Grand Challenge} where hardware and software are developed to autonomously disassemble and sort electronic waste, see~\url{https://automatica-munich.com/en/munich-i/robothon/} .}. Notably, the prospect of future EAI agents autonomously fabricating other EAI agents intertwines this challenge directly with the realms of challenges C1 and C2.

% SUBSECTION ========================================================================================
\paragraph*{\textbf{Tackling the challenges}}
One idea seems to lurk behind the three challenges: \say{more EAI agents, more energy demand}. Indeed, the trends in Fig.~\ref{fig:energy_demands_AI_robotics} suggest that the CCE, BEE, and MIE expenditures will follow a similar pattern. Ongoing efforts to minimize BEE and improve the MIE include works like \cite{schroder2014, chalmers2015, mohammed2014, chemnitz2011}; which advocate strategies such as elastic actuation and optimized hardware selection and storage, energy sharing, and motion planning. As for CCE, it is important to define sample-efficient learning algorithms that account for the constant learning need in EAI agents. 

Our stance is that achieving more efficient energy utilization in EAI goes beyond merely optimizing the bodies or learning strategies of individual agents; the true breakthrough lies in formulating strategies that harness the vast populations of robots to effectively utilize the diverse knowledge they accumulate. Such an approach could alleviate the CCE and MIE burdens by promoting and leveraging concurrent knowledge exchange, ultimately reducing the computational and mechanical energy expended in acquiring new skills. This uncharted terrain prompts consideration of the collective learning paradigm introduced in \cite{Haddadin2014SystemzumErstellen,Haddadin2015Systemgeneratingsets} as a means to harness scalability and facilitate knowledge exchange, thereby promoting energy efficiency in the realm of EAI.

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Modeling the dynamics of skill knowledge}\label{sec:knowledge_dynamics_model}
Understanding the energy and time demands represented by a team of $m$ robots learning a universe $\mathcal{S}=\left\lbrace s_1,s_2,\ldots s_j,\ldots, s_{N_\mathcal{S}}\right\rbrace$ of skills, with $|\mathcal{S}| = N_\mathcal{S}$, requires looking at how knowledge about a skill is gained and what effect it can have on the acquisition of any new skill knowledge. 

To start, we consider that the \emph{complexity} $c_j$ of a skill $ s_j $ is the number of trial episodes $n$ needed to successfully learn the skill, i.e., all actions and states visited by an EAI agent until a stopping criterion is reached. Additionally, %where the system resembles the following behavior.
% ---
% \begin{tcolorbox}
% 	\begin{assumption}\label{assumption:time}
% 		Given a large enough number of robots performing a large number of skills, on average, the power $P_0$ required by any given robot during learning and the time $\Delta t$ allocated to the execution of every trial episode $n$ are approximately constant, see Fig.~\ref{fig:power_per_episode}.
% 	\end{assumption}
% \end{tcolorbox}
\begin{tcolorbox}
	\begin{assumption}\label{assumption:time}
		the average behavior of a system where both $m$ and $N_\mathcal{S}$ are large can be described by the power $P_0$ required by any agent during learning and the mean execution time $\Delta t$ of every trial episode $n$, with both approximately constant; see Fig.~\ref{fig:power_per_episode}.
	\end{assumption}
\end{tcolorbox}
% ---
\noindent As a consequence of Asm.~\ref{assumption:time}, and according to Eqs.~\eqref{eq:energy_per_episode},\eqref{eq:energy_per_skill}, and \eqref{eq:total_energy} in Sec.~\ref{sec:power_per_episode}, the energy demand of an EAI agent learning a skill (or set of skills) is proportional to the skill(s) complexity.

% ===================================================================================================
\paragraph*{Similarity and knowledge}
Let $\mathcal{Z}_k \subset \mathcal{S}$ be a subset of $N_{\mathcal{Z}_k}$ skills that share high similarity; i.e., a \emph{cluster} of similar skills, see Fig.~\ref{fig:skill_similarity}. Furthermore, consider a second set $\mathcal{\zeta}_k \subset \mathcal{Z}_k$ that denotes already learned skills from $\mathcal{Z}_k$. Furthermore, the following assumption is made.  
%---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:skill_clustering} If the similarity among a set of skills is significant, exchanging acquired knowledge from these skills expedites the overall learning process.
	\end{assumption}
\end{tcolorbox}
% ---
\noindent This implies that the $j$-th skill in the $k$-th cluster $s_{j,k} \in \mathcal{Z}_k$ can always benefit from the knowledge contained in $\mathcal{\zeta}_k$. Consequently, the more skills in $\mathcal{\zeta}_k$, the less knowledge about $ s_{j,k} $ remains to be learned. To model this effect, we introduce a function $\bar{\sigma}_{j,k}\left(n\right)\in [0,1]$ that expresses the knowledge about a skill $s_{j,k} \in \mathcal{Z}_k \setminus \mathcal{\zeta}_k$ that \emph{is not} contained in the knowledge base of $\mathcal{\zeta}_k$. The function $\bar{\sigma}_{j,k}(\cdot)$ satisfies
% ---
\begin{equation}\label{eq:sigma_bar_conditions}
	\bar{\sigma}_{j,k}\left(n\right) = 
	\begin{cases}
		1 & \text{$\mathcal{\zeta}_k=\emptyset$},\\
		0 &\text{$\mathcal{\zeta}_k$ has \emph{all} knowledge of $s_{j,k}$}.
	\end{cases}
\end{equation}
% ---
Conceptually, $\bar{\sigma}_ {j,k}\left(\cdot\right)$ is the fraction of knowledge from ${\mathcal{Z}_k}$ that remains to be learned.
% ---
\begin{figure*}[!t]
	\centering
	\hspace*{\fill}
	\begin{subfigure}[t]{0.45\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{skill_similarity.png} \label{fig:skill_similarity}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{knowledge_idealization.png} \label{fig:knowledge_idealization}
	\end{subfigure}
	\hspace*{\fill}
	\caption[] {\label{fig:experimental_results} \textbf{Skill similarity and knowledge.} \subref{fig:skill_similarity} Similar skills in $\mathcal{S}$ can be grouped into clusters $\mathcal{Z}_k$, \subref{fig:knowledge_idealization} remaining knowledge to learn a new skill $s_{j,k}$.}	
\end{figure*}
% ---
% ===================================================================================================
\paragraph*{Leveraging the acquired knowledge}
To evaluate the effect of knowledge exchange during learning on the complexity of mastering a skill, we introduce an upper bound called the skill \textit{fundamental complexity} $c_0$, which describes the maximum number of trial episodes required to learn \emph{any} skill. If, in learning a skill $ s_{j,k} $, an EAI agent can access and use the knowledge contained in $\mathcal{\zeta}_k$; then, two effects take place:
% ---
\begin{enumerate}
	\item There is less remaining knowledge, reflected in the initial value; i.e. $\bar{\sigma}_{j,k}(0) < 1$
	\item The knowledge acquisition rate increases
\end{enumerate}
% ---
%associated complexity $ c_{j,k} $ is necessarily smaller than the fundamental complexity $c_{0}$; i.e. $c_{j,k} < c_0~\forall j>1$.
These effects signify that the remaining knowledge scales down as a function of the number of learned skills $N_{\zeta_k}=|\mathcal{\zeta}_k|$. As a consequence, the complexity $c_{j,k}$ of said skill is smaller than the fundamental complexity $c_0$. Additionally, without loss of generality, under knowledge exchange we can consider that
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:exponential_decrease} the remaining knowledge function $\bar{\sigma}_{j,k}(\cdot)$ has a monotonically decreasing behavior.
	\end{assumption}
\end{tcolorbox} 
% ---
\noindent An idealization of the behavior satisfying Asm.~\ref{assumption:exponential_decrease} and Eq.~\eqref{eq:sigma_bar_conditions} can be modeled via a differential equation depending on the trial episodes $n$ and is parameterized by the number of already learned skills $N_{\zeta_k}$. As such,
% ---
\begin{definition}\label{assumption:ode_model} the remaining knowledge function $\bar{\sigma}_{j,k}$ is modeled as the first order dynamical system
	\begin{subequations}\label{eq:simple_knowledge_dynamics}
		\begin{empheq}[left=\empheqlbrace]{align}
			\dot{\bar{\sigma}}_{j,k}\left(n\right) &  = -f_{j,k} \left(N_{\zeta_k} \right) \bar{\sigma}_{j,k}\left(n\right),\\
			\bar{\sigma}_{j,k}(0) &  =  g_{j,k} \left(N_{\zeta_k}\right).
		\end{empheq}
	\end{subequations}
\end{definition}
% ---
\noindent Its solution
% ---
\begin{equation}\label{eq:knowledge_exponential_form}
	\bar{\sigma}_{j,k}(n) = g_{j,k}\left(N_{\zeta_k}\right) e ^{-f_{j,k}\left(N_{\zeta_k}\right) n} \in (0,1],
\end{equation}
% ---
exhibits the desired behavior, shown in Fig.~\ref{fig:knowledge_idealization}. The function $f_{j,k}\left(N_{\zeta_k}\right)$ models one of the effects resulting from the exploitation of the knowledge available in $\zeta_k$, namely, the increase of the learning rate. The second effect, i.e. the reduction in the initial remaining knowledge $\bar{\sigma}_{j,k}(0)$ is controlled by the term $g_{j,k}\left(N_{\zeta_k}\right)$, which is also dependent on the number of learned skills. The learning threshold $\epsilon$ in Fig.~\ref{fig:knowledge_idealization} indicates when the remaining knowledge is negligible and $s_{j,k}$ is considered as learned.
% ---
%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=0.45\textwidth]{fig/knowledge_idealization.png}
%	\caption{Remaining knowledge to learn a new skill $s_{j,k}$.}
%	\label{fig:knowledge_idealization}
%\end{figure}
% ---

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Knowledge sharing under different learning paradigms}
% ---
\begin{figure*}[!t]
	\centering
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{intra_skill_learning.png} \label{fig:intra_skill_learning}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{cluster_to_cluster_knowledge_transfer_parallel.png} \label{fig:cluster_to_cluster_knowledge_transfer_parallel}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{cl_example_figure.png} \label{fig:cl_example_figure}
	\end{subfigure}	
	\hspace*{\fill}
	\caption[] {\label{fig:learning_paradigms_conceptual_figure} \textbf{The different learning paradigms.} \subref{fig:intra_skill_learning} Incremental learning benefits from the significant similarity of skills belonging to the same cluster. \subref{fig:cluster_to_cluster_knowledge_transfer_parallel} In transfer learning knowledge is shared from different origin clusters to the target cluster, notice that using many robots (e.g. two robots $r_1$ and $r_2$) without inter-agent knowledge exchange among them only subdivides the problem. \subref{fig:cl_example_figure} Exchange of knowledge between EAI agents enables collective learning.}
\end{figure*}
% ---

For the upcoming analysis we consider an idealized reference system in which a large number of robots coexist learning a large number of skills. Such system exhibits
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:average_behavior}
		an average behavior that results from comparable EAI agents learning and executing the skills in $\mathcal{S}$ ordered and segregated according to their similarity.
	\end{assumption}
\end{tcolorbox}
%---
\noindent Each of the EAI agents in the system
\begin{tcolorbox}
	\begin{assumption}\label{assumption:agent_similarity}
		has the same capabilities with highly similar BEE and MIE expenditures.
	\end{assumption}
\end{tcolorbox}
%---
\noindent The large number of skills in $\mathcal{S}$ implies that
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:cluster_size}
		every cluster $\mathcal{Z}_{k}$ contains the same number $N_{\mathcal{Z}} $ of skills.
	\end{assumption}
\end{tcolorbox}
% ---
\noindent By virtue of the optimal ordering of the skills and the balanced size of the clusters,
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:cluster_transferability}
		the knowledge transferability between in-cluster skills ---modeled by \eqref{eq:f_function_incremental} and \eqref{eq:g_function_incremental}--- is assumed to be equal; as is transferability between clusters, see \eqref{eq:f_function_transfer} and \eqref{eq:g_function_transfer}.
	\end{assumption}
\end{tcolorbox}
% ---
\noindent Finally, the different learning paradigms that exploit the collected knowledge by the EAI agents rely on the fact that
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:enabling_agorithms}
		there are advanced control and machine learning algorithms designed to inherently use this knowledge.
	\end{assumption}
\end{tcolorbox}
% ---

% ===================================================================================================
\paragraph*{Conventional learning paradigms} 
%Now we briefly go over the fundamental aspects of the different learning paradigms considered for EAI, a detailed discussion is provided in \nameref{sec:materials_and_methods}. 
When an EAI agent performs \textbf{isolated learning} (IsL), it learns each new skill from the ground up, disregarding the accumulating knowledge from already learned skills. In contrast, \textbf{incremental Learning} (IL) corresponds to the case where an agent benefits from the continuous aggregation and exchange of knowledge from \emph{intra-cluster} skills in virtue of their significant similarity. As depicted in Fig.~\ref{fig:intra_skill_learning}, a robot ($r_1$ in this case) learns every skill in $\mathcal{Z}_1$ with a rate $\alpha$ ---the self-loops --- but also retains and uses the acquired knowledge to learn subsequent skills. \textbf{Transfer learning} (TL) alone refers to the one-time \emph{inter-cluster} exchange of knowledge. TL represents the exchange of knowledge from the skills learned in different \emph{origin} clusters $\mathcal{O} = \{ \mathcal{Z}_1,\mathcal{Z}_2,\ldots,\mathcal{Z}_{k-1} \}$ to the skills that will be learned in a \emph{destination} cluster $\mathcal{Z}_k$ (see Fig.~\ref{fig:cluster_to_cluster_knowledge_transfer_parallel}). Concretely, the effect that TL has on the skills of the destination cluster is the reduction of the initial remaining knowledge and the increase of the initial learning rate for all the skills in the $k$-th cluster via the parameter $\beta_k$. In general, transfer learning is always complemented with IL, and so we define the combination of both \textbf{transfer with incremental learning} (TIL) as the third learning paradigm.  %In all these paradigms, there is no exchange of knowledge among agents, which implies that employing $m$ agents in parallel to learn $m$ skills only subdivides the problem and does not provide an advantage. 
A detailed discussion on the effects that these paradigms have on the skill complexity is provided in Sec.~\ref{sec:materials_and_methods}.

% ===================================================================================================
\paragraph*{\textbf{Collective learning (CL)}}
This paradigm  goes beyond simple parallelization. In CL $m$ robotic agents $ \left\lbrace r_i \right\rbrace_{i=1}^{m} $  develop and accumulate a common mind (body of knowledge) dynamically via networked interactions where individual experience, knowledge, and skills are disseminated to all the other elements in the collective. Information flows vertically as previous knowledge is passed on and horizontally by sharing concurrent experience between agents. Knowledge can be replicated, complemented, and further developed via these mechanisms. We take from \cite{Garavan2012CollectiveLearning} two notions central in CL that apply to EAI agents:
$(I)$ capability to restructure and meet changing conditions, and $(II)$ aggregation of skills, knowledge, and behaviors. Moreover, to enable CL, it is assumed that an inter-agent communication protocol and the appropriate infrastructure are in place that enables agents to concurrently exchange and integrate the self-acquired and received knowledge to incrementally speed up the learning of all the agents as a whole. As a result, intra- and inter-cluster knowledge transfer is possible. Naturally, the CL paradigm involves a complex scheduling problem to determine the optimal skill distribution and inter-agent knowledge-sharing strategy. 

\myhl{Recent contributions have partially addressed aspects of the CL paradigm, as exemplified by works such as \cite{levine2018learning, rudin2022learning, flairop2023}. However, it is essential to clarify the focus of this article: we do not delve into the specifics of algorithms required for CL, as many of them are still nonexistent or under development, nor do we explore the necessary advancements in processing and communication infrastructure to enable CL. Instead, our primary objective is to illustrate the overarching systemic behavior inherent in the CL paradigm, grounded on Assumptions~\ref{assumption:average_behavior}, \ref{assumption:agent_similarity},~\ref{assumption:cluster_size}, and~\ref{assumption:cluster_transferability}; particularly,  regarding its target knowledge-sharing dynamics.} 

Fig.~\ref{fig:cl_example_figure} illustrates the CL concept, where the self-loop represents the dynamics of a single robot learning at a rate $\alpha$. The exchange of knowledge across agents is represented via the cross-couplings weighted by a parameter $\gamma$ that models how efficient is the bidirectional pairwise knowledge exchange. Similar to TL, if two robots exchange knowledge about skills with low similarity, i.e., skills in different clusters, then $\gamma$ is scaled by the inter-cluster transferability parameter $\beta$. In CL the dynamics of the remaining knowledge is described by
% ---
\begin{subequations}\label{eq:collective_knowledge_dynamics}
	\begin{empheq}[left=\empheqlbrace]{align}
		\dot{\bar{\bm{\sigma}}}^{(CL)}_{j,k}\left(n\right) &= \left[  h_{j,k}\left(N_{\zeta_k},r\right) \bm{I} + \gamma \bm{A} \odot \bm{B}  \right] \bar{\bm{\sigma}}^{(CL)}_{j,k}\left(n\right)\\
		\bar{\bm{\sigma}}^{(CL)}_{j,k}(0) &= g_{j,k}\left( N_{\zeta_k}, r\right) \bm{I},
	\end{empheq}
\end{subequations}
% ---
where $r=m$ is the number of robots that exchange knowledge among them. Now, $\bar{\bm{\sigma}}^{}_{j,k} \in \mathbb{R}^r$ is a vector that represents the dynamics of the remaining knowledge of all the $m$ skills being concurrently learned. $\bm{A} \in \mathbb{R}^{r \times r}$ is a zero-diagonal symmetric adjacency matrix whose entry $(\bm{A})_{i,j} = 1$ if robot $r_i$ exchanges knowledge with robot $r_j$ and $(\bm{A})_{i,j} = 0$ if it does not. The term $\gamma \in \mathbb{R}_+ $ weighs the knowledge exchange strength among robots. Since there may be robots learning skills in different clusters at the same time, the matrix $\bm{B}$, whose entries are $\left(\bm{B}\right)_{i,j} \in \left \lbrace 1, \beta_{k} \right \rbrace$, with
% ---
\begin{equation}
	%\beta_{k} = 1/N_\mathcal{K}, 
	\beta_{k} = r\frac{ N_{\zeta_k}}{N_\mathcal{S}}, 
\end{equation}
% ---
scales down the knowledge contributions between robots from different clusters. Finally, the operator $\odot$ represents the Hadamard product of matrices. The functions $ h(\cdot)$ and $g(\cdot)$ in Eq.~\eqref{eq:collective_knowledge_dynamics}, with the former defined as
% ---
\begin{equation}\label{eq:f_function_collective}
	h_{j,k}\left(N_{\zeta_k},r\right) = -\alpha \left( \frac{\eta r N_{\zeta_k} + 1}{1 - \beta_k} \right),
\end{equation}
% --- 
are dependent on the number of knowledge-exchanging robots, which directly impacts the number of skills that enter $\zeta_k$ after a learning cycle.

The dynamics of the remaining knowledge for the considered learning paradigms are described by replacing in Eq.~\eqref{eq:simple_knowledge_dynamics} the expressions for the rate of remaining knowledge and the corresponding initial value as per Table~\ref{tab:learning_paradigms_expressions}; where
% ---
\begin{enumerate}
	\item the constant $ \alpha>0$ models the rate at which a robot in isolation learns any given skill,
	\item the constant $\eta>0$ represents the efficiency of knowledge exchange from $\zeta_k$ to $s_{j,k}$,
	\item the factor $\delta>0$ controls the rate of exponential decrease in the initial remaining knowledge value, and
	\item $\beta_k$ is the head start granted by knowledge transfer from other clusters to the skills in $\mathcal{Z}_k$.
\end{enumerate}
% ---
%\begin{table}[!ht]
%	\caption{The dynamics of classical learning paradigms.\label{tab:learning_paradigms_expressions}}
%	\begin{center}
%		\begin{adjustbox}{width=\textwidth}
%			\begin{tabular}{ |c|c|c|c|c|} 
%				\hline
%				Learning type & IsL & IL & TIL & CL  \\
%				\hline
%				Rate $f_{j,k}\left(\cdot \right)$ & $ \alpha$ & $ \alpha\left(\eta N_{\zeta_k} + 1 \right)$ & $\alpha \left( \frac{\eta N_{\zeta_k} + 1}{1 - \beta_k} \right)$ & $h_{j,k}\left(N_{\zeta_k},r\right) \bm{I} + \gamma \bm{A} \odot \bm{B}$ \\[5ex]
%				\hline
%				Initial condition $g_{j,k}\left(\cdot \right)$  & $1$ & $e^{-\delta N_{\zeta_k}}$ & $(1-\beta_k) e^{-\delta N_{\zeta_k}}$ & $(1-\beta_k) e^{-\delta r N_{\zeta_k}} \bm{I}$\\[5ex]
%				\hline
%			\end{tabular}
%		\end{adjustbox}
%	\end{center}	
%\end{table}
% ---

\begin{table}[!ht]
\caption{The dynamics of the learning paradigms.\label{tab:learning_paradigms_expressions}}
\begin{center}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{|l||*{4}{c|}}\hline
	Learning type
	&\makebox[3em]{IsL}&\makebox[3em]{IL}&\makebox[3em]{TIL}
	&\makebox[3em]{CL}\\\hline\hline
	Rate $f_{j,k}\left(\cdot \right)$  &$ \alpha$ & $ \alpha\left(\eta N_{\zeta_k} + 1 \right)$ & $\alpha \left( \frac{\eta N_{\zeta_k} + 1}{1 - \beta_k} \right)$ & $h_{j,k}\left(N_{\zeta_k},r\right) \bm{I} + \gamma \bm{A} \odot \bm{B}$ \\\hline
	Initial condition $g_{j,k}\left(\cdot \right)$ &$1$ & $e^{-\delta N_{\zeta_k}}$ & $(1-\beta_k) e^{-\delta N_{\zeta_k}}$ & $(1-\beta_k) e^{-\delta r N_{\zeta_k}} \bm{I}$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}	
\end{table}
% ---
% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Results}\label{sec_use_case}

\myhl{Explain that this example is equally valid for scenario x,y,z. Or one says, we consider the following scenarios (parameter wise), which applies to a smart factory scenario where ..., or a home community scenario where..., or a underwater exploration scenario where...., or space exploration scenario where.... It should be noted that it is a usecacse that basically transfer to others as well, ii.e., it has significant meaning to arbitrary domains.}

To show the advantages of using CL on reducing the complexity of skill learning and thereby reducing the energy consumed by learning the skills in $\mathcal{S} $, we contextualize the problem in a hypothetical case study contrasting different learning paradigms in terms of their associated energy demand (related to the CCE, BEE, and MIE expenditure categories). Concretely, we estimate the substantial effect that CL can have on energy and time demands in a smart factory scenario with several robots performing multiple skills, showing that CL is the natural solution to address the energy demand problem in EAI.

We use a prototypical instance of a hypothetical smart factory with $m=32$ robots to assess the potential energy demand that EAI agents could have relative to the use of the previously discussed learning paradigms. In this mock factory a pool $\mathcal{S}$ of $N_\mathcal{S}= 512$ skills is divided into $N_\mathcal{K}=4$ clusters of $N_\mathcal{Z} = 128$ skills each. Additionally, when the remaining knowledge goes below $\epsilon = 0.01$ a given skill is considered learned. For all the skills, the fundamental skill complexity is $c_0 = 100$ episodes.

The power-per-episode (see Eq.~\eqref{eq:energy_per_episode}) is determined by the sum of the power required for basal processes, the power for motion and interaction, and the power for computation and communication, i.e.
% ---
\begin{equation}
	P_0 = P_{BEE}+P_{MIE} + P_{CCE}.
\end{equation}
% ---
To choose $P_{BEE}$, we consider that the smart factory will be populated with state-of-the-art tactile robots, like those listed in Sec.~\ref{sec:app_cobot_ener_consumption}, which require a typical power of about $\unit[40]{W}$. To approximate $P_{MIE}$, we use the fact that, in demanding tasks, the power demand of a cobot can go up to a maximum of about $ \unit[300] {W} $. Finally, to determine $P_{CCE}$, we assume that, to deal with the computing effort that learning new skills will have on the robots' local processors, the smart factory will delegate the computational burden to a remote computing unit, i.e., cloud computing. Thus, we take as reference the work in \cite{Strubell2019EnergyPolicyConsiderations}, where a state-of-the-art machine learning algorithm executed in a cluster required $\unit[1,415.78]{W}$ to solve a task. Without loss of generality, we can assume that each trial episode $n$ takes $\Delta t = 60$ seconds to execute. Using these reference values, we can estimate that, when learning a skill, a trial episode has an energetic demand of:
% ---
\begin{equation}
	e_0 = P_0 \Delta t = \left(40 + 300 + 1,415.78\right) \left(60\right) \approx 105~\text{kJ}.
\end{equation}
% ---
Regarding the knowledge exchange efficiency constants, they are chosen to be $\alpha =  0.0461$ (in accordance to Eq.~\eqref{eq:isolated_learning_rate}), $\delta =  0.0360$ (see Eq.~\eqref{eq:delta}), and $\eta= 0.1$.
% ===================================================================================================
\paragraph*{The skill complexity of the different paradigms}
The remaining knowledge for the four skills learned per robot is shown in Fig.~\ref{fig:collective_learning} in logarithmic scale. The $m$ robots are used to learn in parallel the $N_\mathcal{Z}$ skills of each cluster in succession, as shown in Fig.~\ref{fig:cluster_learning_sequence}. Notice that, as expected, IsL (Fig.~\ref{fig:dynamics_isolated_learning}) exhibits the worst performance, always requiring $c_0$ episodes to learn every skill. Since IL (Fig.~\ref{fig:dynamics_incremental_learning}) does not benefit from the knowledge from the previously visited clusters, a robot $r_i$ needs to start accumulating knowledge from the beginning every time it moves to a different cluster. This is not the case in TIL (Fig.~\ref{fig:dynamics_incremental_transfer_learning}), as the more clusters a robot has visited, the faster a new skill is learned. The speed of knowledge collection is exponentiated with CL (Fig.~\ref{fig:dynamics_collective_learning}) thanks to the exchange of knowledge among the $m$ robots. Compared to the other learning paradigms, with CL, the skills are learned within a few trial episodes in every cluster. 

To assess how the number $m$ of robots affects the total number of trial episodes $C_\mathcal{S}$ required to learn all the $N_\mathcal{S}$ skills, we use the same parameters as before but vary $m \in \left \lbrace 2,4,8,16,32,64,128\right \rbrace$. Moreover, we considered an additional CL scenario in which, unlike the previous case, the total number of available robots is distributed equally among the clusters to benefit from transfer learning at an earlier time during learning. The results are shown in Fig.~\ref{fig:total_episodes_per_n_robots}. It can be seen that, at first, IL is better than the trivial IsL case; however, as the number of robots increases, the skill knowledge is divided among the available robots, which implies that less knowledge can be passed as the pool of learned sills $\zeta_k$ per robot gets smaller. This explains why the total number of trial episodes for IsL and IL approach each other in the limit. With a growing robot number, TIL exhibits a similar behavior. Less cluster knowledge can be collected by each robot and transferred to the next cluster. Indeed, TIL rapidly converges to IL and eventually to IsL. In CL, a similar effect shows that when all robots learn skills from the same clusters, as the number of robots grows, the total complexity approaches that of the same number of robots distributed across clusters.
% ---
\begin{figure*}[!h]
	\centering
	\hspace*{\fill}
	\begin{subfigure}[t]{0.45\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{total_episodes_per_n_robots.png} \label{fig:total_episodes_per_n_robots}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.45\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{total_energy_per_n_robots.png} \label{fig:total_energy_per_n_robots}
	\end{subfigure}
	\hspace*{\fill}
	\caption[] {\label{fig:final_results} The effect of the number of robots: \subref{fig:total_episodes_per_n_robots} Total number of episodes to learn the universe of skills as a function of the available robots and \subref{fig:total_energy_per_n_robots} the total energy consumption.}
\end{figure*}
% ---

% SUBSECTION ========================================================================================
\paragraph*{Energy consumption}
Consider that the results in Fig.~\ref{fig:total_episodes_per_n_robots} show the total number of episodes required by each of the $m$ robots. To compute the total energy demand, those numbers need to be scaled by the factor $m e_0$, which leads to the consumption shown in Fig.~\ref{fig:total_energy_per_n_robots}. Undoubtedly, CL shows that it has not only the best energy usage of all the paradigms but, unlike the rest, the more robots take part in learning the universe of skills, the better overall energy usage.

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Discussion}\label{sec:discussion}
While the unprecedented strides in AI and robotics have revolutionized numerous sectors, the ongoing integration of EAI agents and their energy consumption cannot be ignored. As the scope of EAI continues to expand, a concerted effort is required to strike a balance between innovation and conscientious energy usage to steer EAI toward sustainable operation. To emphasize the energy consumption's significance in EAI systems, we introduced three principal energy expenditure categories. We juxtaposed them with the grand challenges stemming from the escalating population of EAI agents. We underscored that mitigating energy consumption in EAI systems requires enhanced mechanical designs, efficient computation and communication hardware, and a paramount emphasis on harnessing the simultaneous sharing, exchange, transfer, and accumulation of computational knowledge acquired by the various agents.

% ===================================================================================================
\paragraph*{Developing collective learning to address the energy challenges of EAI}
The energy grand challenges associated with EAI have natural connections between them; see Fig.~	\ref{fig:challengesConnected}. This allows us to identify important areas of opportunity for collective learning. Directly related to challenge C1 and the computation and communication energy expenditure ($E_{CCE}$), the EAI research community stands to gain significant headway by channeling efforts into realizing collective learning algorithms. This might involve focusing on data-efficient methodologies, infusing pertinent prior knowledge into models, or fostering knowledge-sharing capabilities. The latter emerges as an especially remarkable solution (as shown in our simulation study), poised to expedite multi-skill learning by leveraging a cloud-connected repository of skills. With time, this paradigm shift could transform data center demands from compute-intensive to storage and querying, drastically reducing energy needs, apart from the compelling need to advance computational algorithms to leverage efficient hardware and streamline communication protocols. Challenge C2 is intricately interwoven with the surging population of active robots and other intelligent machines. The relevance of better mechatronic designs (e.g., lightweight materials, flexible components, and energy-efficient actuation) for fine-tuning energy utilization during skill execution is obvious and a problem by itself. Collective learning can contribute to reducing the basal energy expenditure ($E_{BEE}$) and the energy implicated in motion and interaction ($E_{MIE}$) by reducing the time dedicated to learning and executing skills thanks to the body of knowledge collected by the multitude of agents with similar capabilities. Finally, the efforts dedicated to approaching challenges C1 and C2 will ripple into advancements in C3. On a side note, recycling is a supplementary avenue for energy optimization in C3. Integrating recycling within the manufacturing landscape of machines would enable the reclamation of usable parts from retired robots and the reutilization of materials from discarded components. As the journey towards energy-efficient EAI unfolds, a holistic approach marrying innovative algorithms, efficient hardware, sustainable designs, and recycling endeavors offers a beacon of promise.
% ---
\begin{figure}[!t]
	\centering
	\includegraphics[width=0.45\textwidth]{fig/grand_challenges_connections.png}
	\caption{Interconnection between challenges C1, C2, and C3.}
	\label{fig:challengesConnected}
\end{figure}
% ---

% ===================================================================================================
\paragraph*{Closing remarks}
Our case study findings revealed that utilizing established paradigms such as isolated, incremental, and transfer learning on a large number of EAI agents does not lead to optimal energy utilization. This remains true even when multiple agents operate concurrently, as there's an absence of inter-agent knowledge exchange. Moreover, our results indicate a substantial increase in energy requirements under conventional learning paradigms as EAI agent usage grows. Conversely, our simulation study highlighted that collective learning presents a solution to energy demands contingent on effectively sharing knowledge based on skill similarity. Notably, the collective learning approach showcased superior performance with increasing robot numbers, enabling concurrent acquisition of multiple skills.


As discussed in \cite{Kaelbling2020foundationefficientrobot}, efficient robotic learning algorithms capable of enabling agents to acquire new skills on the fly must possess certain key attributes: sample efficiency, generalizability, compositionality, and incremental learning capabilities. The Collective Learning paradigm inherently fulfills these prerequisites by harnessing the full communication potential of networked EAI (Embodied Artificial Intelligence) agents. This approach facilitates real-time collaborative knowledge exchange and aggregation, resulting in energy- and time-efficient skill acquisition.

\myhl{Despite the potential advantages of collective learning, it is important to note that the algorithms and infrastructure essential for CL are either non-existent or still in the developmental stages. Furthermore, the state-of-the-art algorithms that strive for incremental and transfer learning are currently in their infancy. Nevertheless, considering that CL not only addresses the formidable challenges posed by EAI but also offers a potential unified solution to broader learning problems, we hope that the arguments presented in this discussion will inspire further research efforts toward the realization of collective learning.}

% As discussed in \cite{Kaelbling2020foundationefficientrobot}, a learning algorithm enabling agents to learn new tasks on the fly must be sample-efficient, generalizable, compositional, and incremental. The collective Learning paradigm naturally meets these requirements, tapping into the full communication potential of networked EAI agents to facilitate real-time synergistic exchange and aggregation of knowledge to achieve energy- and time-efficient skill learning. \myhl{All the potential of CL aside, it is pertinent to mention that the algorithms and infrastructure required for CL are either nonexistent or under development. Moreover, state-of-the-art algorithms that aim for incremental and transfer learning are still in their infancy. Yet, as CL promises not only to tackle the significant challenges posed by EAI, but also posits a potential unifying solution for the general learning problem, we hope the arguments discussed here foster the research to realize collective learning.}

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Supplementary Materials}
\TODO{}
%Sections \ref{sec:learning_b_schema} to \ref{sec:lack_of_excitation}\\
%Fig.~\ref{fig:learning_self_pipeline} to Fig.~\ref{fig:fs10_proximal_links_excitation}

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\renewcommand\refname{References}
\bibliography{bib/References.bib}%,
\bibliographystyle{Science}
%\begin{thebibliography}{10}
%	
%\end{thebibliography}

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\textbf{Acknowledgments:} \TODO{}
%We thank Fan Wu from the Munich Institute of Robotics and Machine Intelligence for his feedback and support throughout the research process. \textbf{Funding:} The authors greatly acknowledge the funding of this work by the Alfried Krupp von Bohlen und Halbach Foundation, the Lighthouse Initiative Geriatronics by StMWi Bayern (Project X, grant no. 5140951), the Lighthouse Initiative KI.FABRIK Bayern by StMWi Bayern, Forschungs- und Entwicklungsprojekt, grant no. DIK0249 and Phase 1: Aufbau Infrastruktur, as well as the Federal Ministry of Education and Research of the Federal Republic of Germany (BMBF), by funding the project AI.D under the Project Number 16ME0539K. \textbf{Author contributions:} The concepts were developed by  F. Daz Ledezma and S. Haddadin. F. Daz Ledezma implemented and conducted all of the experiments and analyzed the data. F. Daz Ledezma and S. Haddadin interpreted the results. F. Daz Ledezma and S. Haddadin conceptualized, F. Daz Ledezma wrote, and S. Haddadin revised and edited the manuscript. All of the authors read the paper. \textbf{Competing interests:} Please note that S. Haddadin has a potential conflict of interest as a shareholder of Franka Emika GmbH, the manufacturer of the used robot manipulator. \textbf{Data and materials availability:} All data needed to evaluate the conclusions in the paper are present in the main manuscript or the Supplementary Materials. The datasets generated and analyzed in the current study are available at \url{https://github.com/mecafdl/pigraphs_body_morphology}. Requests for additional materials should be addressed to S. Haddadin.

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
 \newpage
 \beginsupplement
 \section*{Supplementary Materials}\label{sec:supplementary_materials}
 \input{supplementary.tex}

\end{document}