% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt]{article}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% http://www.sciencemag.org/authors/preparing-manuscripts-using-latex 
% This package should properly format in-text
% reference calls and reference-list numbers.

\usepackage{adjustbox}
\usepackage{scicite}

\usepackage{times}
\usepackage{units}

%\usepackage[T1]{fontenc}
%\usepackage[ngerman]{babel}
\usepackage[english]{babel}
\usepackage{empheq}

\usepackage[]{graphicx}
\graphicspath{ {./fig/} }
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
\usepackage[labelformat=simple]{subcaption}  
\captionsetup[subfigure]{font={bf,small}, skip=1pt, margin=-0.1cm, singlelinecheck=false}
\usepackage[labelfont=bf]{caption}
\captionsetup{labelfont=bf}
\captionsetup{font=footnotesize}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{dirtytalk}
%\usepackage{fourier}
\usepackage{siunitx}
\usepackage{tcolorbox}
\usepackage{wrapfig}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

% Added by authors
\usepackage{siunitx}
\usepackage{tabularx,ragged2e,booktabs}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{bm}
\usepackage{mathtools}
\captionsetup[figure]{name={Fig.},labelsep=period}
%\captionsetup[table]{name={Table},labelsep=period}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{xr}
\externaldocument{supplementary_materials}


%\usepackage[demo]{graphicx}
%\usepackage{ifdraft}
%\ifdraft{\renewcommand{\includegraphics}{\relax}}{\relax}
%\usepackage{comment}
%\excludecomment{figure}
%\let\endfigure\relax


\newcommand\hl[1]{\colorbox{yellow}{\textcolor{red}{#1}}}
\newcommand\myhl[1]{\textcolor{red}{#1}}



% Use this to display line numnbers
\usepackage{lineno}
\linenumbers

\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\renewcommand{\emph}[1]{\textit{#1}}
\let\textcircledold\textcircled

\renewcommand{\textcircled}[1]{\raisebox{.5pt}{\textcircledold{\raisebox{-.45pt} {#1}}}}
\newcommand{\pigraph}{$\pi$-graph}
\newcommand*{\important}[1]{\textcolor{red}{\danger~\textbf{IMPORTANT:~}} \textcolor{red}{#1}}
\newcommand*{\pending}[1]{\textcolor{blue}{$\bigstar$~\textbf{PENDING~#1}}}
\newcommand\mybox[2][]{\tikz[overlay]\node[fill=blue!100,inner sep=4pt, anchor=text, rectangle, rounded corners=1mm,#1] {#2};\phantom{#2}}

\newcommand{\TODO}[1]{\mybox[fill=yellow]{\textcolor{blue}{\Large \textbf{TODO}}:~\textcolor{blue}{\textbf{\emph{#1}}}}}
\newcommand{\xmark}{\ding{55}}%
\newcommand{\textcircledD}[1]{\raisebox{.9pt}{\textcircled{\raisebox{+.5pt} {\footnotesize#1}}}}
\newcommand{\hu}[1]{\textcolor{orange}{[Hu: #1]}}
\newcommand{\kuehn}[1]{\textcolor{blue}{[Kuehn: #1]}}
\newcommand{\diaz}[1]{\textcolor{blue}{[Diaz: #1]}}
\newcommand{\haddadin}[1]{\textcolor{red}{[Haddadin: #1]}}
\newcommand{\del}[1]{\textcolor{orange}{\xout{#1}}}
\newcommand{\new}[1]{\textcolor{orange}{#1}}
\DeclareMathOperator*{\E}{\mathbb{E}}
\renewcommand{\thesubfigure}{\textbf{\Alph{subfigure}}}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}

\renewcommand{\figurename}{Fig.}


%% MY ADDED SECTION
\usetikzlibrary{backgrounds}
\makeatletter

\tikzset{%
	fancy quotes/.style={
		text width=\fq@width pt,
		align=justify,
		inner sep=1em,
		anchor=north west,
		minimum width=\linewidth,
	},
	fancy quotes width/.initial={.8\linewidth},
	fancy quotes marks/.style={
		scale=8,
		text=white,
		inner sep=0pt,
	},
	fancy quotes opening/.style={
		fancy quotes marks,
	},
	fancy quotes closing/.style={
		fancy quotes marks,
	},
	fancy quotes background/.style={
		show background rectangle,
		inner frame xsep=0pt,
		background rectangle/.style={
			fill=gray!25,
			rounded corners,
		},
	}
}

\newenvironment{fancyquotes}[1][]{%
	\noindent
	\tikzpicture[fancy quotes background]
	\node[fancy quotes opening,anchor=north west] (fq@ul) at (0,0) {``};
	\tikz@scan@one@point\pgfutil@firstofone(fq@ul.east)
	\pgfmathsetmacro{\fq@width}{\linewidth - 2*\pgf@x}
	\node[fancy quotes,#1] (fq@txt) at (fq@ul.north west) \bgroup
}
{\egroup;
	\node[overlay,fancy quotes closing,anchor=east] at (fq@txt.south east) {''};
	\endtikzpicture}

\makeatother
\newcommand{\task}{\ensuremath{\tau}}
\newcommand{\sltwoi}{\ensuremath{t_l}} %single learning time without index
\newcommand{\slt}[1]{\ensuremath{t_{l,#1}}} %... with index
\newcommand{\tlt}{\ensuremath{T}} %total learning time
\newcommand{\comp}{\ensuremath{c}} %complexity (learning time from scratch)
\newcommand{\diste}[1]{\ensuremath{\mathrm{d}(\task_{#1},\{ \})}}
\newcommand{\dist}[2]{\ensuremath{\mathrm{d}(\task_{#1},\{\task_1, \task_2, \dots, \task_{#2}\})}}
\newcommand{\En}{\ensuremath{E}}
\newcommand{\opt}{\ensuremath{\mathrm{opt}}}
\newcommand{\tot}{\ensuremath{\mathrm{tot}}}
\newcommand{\Opt}{\ensuremath{\mathrm{Opt}}}
\newcommand{\densMan}{\ensuremath{\rho_{\mathrm{man}}}} %manufacturing energy density
\newcommand{\Tau}{\ensuremath{\mathcal{T}}}

\newcommand{\redtext}[1]{\textcolor{red}{#1}}
\setlength{\columnsep}{1cm}

\newtheorem{challenge}{\textbf{CHALLENGE}}

\renewcommand{\arraystretch}{2} 

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.
\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm


%The next command sets up an environment for the abstract to your paper.
\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}


% Include your paper's title here
\title{\textbf{Title:} Addressing AI Sustainability: Collective Learning for Energy Efficiency}

% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.

\author
{\textbf{Authors:} Fernando D\'iaz Ledezma$^{\ast}$ and Sami Haddadin
	\\
	\normalsize{\textbf{Affiliations:} Chair of Robotics and Systems Intelligence,}\\
	\normalsize{MIRMI - Munich Institute of Robotics and Machine Intelligence,}\\
	\normalsize{Technical University of Munich, Georg-Brauchle-Ring 60-62, M\"unchen, 80992, Germany}\\
	\\
	\normalsize{$^\ast$To whom correspondence should be addressed; E-mail: fernando.diaz@tum.de}
}

% Include the date command, but leave its argument blank.

\date{}



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document} 
% Double-space the manuscript.

\baselineskip24pt

% Make the title.

\maketitle 



% Place your abstract within the special {sciabstract} environment.
\begin{sciabstract}
	\textbf{Abstract:} The current learning paradigms of classical artificial intelligence (AI) consume significant amounts of energy due to high computational loads and limited utilization of acquired knowledge. As AI and robotics merge to form embodied AI (EAI) systems, their energy demand will continue to rise as data acquisition and learning rely on constant interaction with the physical environment. This study examines the fundamental energy requirements of EAI systems and discusses the energy challenges associated with maintaining current learning paradigms. Consequently, we position collective learning, a paradigm shift that enables efficient learning in EAI agents by actively sharing, aggregating, and utilizing previous and current knowledge across systems, as the key to reducing energy consumption and facilitating the acquisition of new skills in shorter timeframes.
\end{sciabstract}

%\textbf{One-Sentence Summary:} Embracing collective learning in (embodied) AI reduces energy consumption and accelerates skill acquisition by orders of magnitude.

% In setting up this template for *Science* papers, we've used both
% the \section* command and the \paragraph* command for topical
% divisions.  Which you use will of course depend on the type of paper
% you're writing.  Review Articles tend to have displayed headings, for
% which \section* is more appropriate; Research Articles, when they have
% formal topical divisions at all, tend to signal them with bold text
% that runs into the paragraph, for which \paragraph* is the right
% choice.  Either way, use the asterisk (*) modifier, as shown, to
% suppress numbering.

%%%%%% Main Text %%%%%%

\newcommand{\beginsupplement}
{%
	\setcounter{table}{0}
	\renewcommand{\thesection}{S\arabic{section}}
	\renewcommand{\thetable}{S\arabic{table}}%
	\setcounter{figure}{0}
	\renewcommand{\thefigure}{S\arabic{figure}}%
}


\section*{Main Text:}

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Introduction}\label{sec:intro}
As research and development in artificial intelligence (AI) progresses, particularly in the field of machine learning, AI-powered technology permeates many aspects of human life. We can expect, for instance, smart factories to become the norm, healthcare services to harness the analytical and predictive capabilities of AI, and households to evolve into predominantly automated environments. The future will also witness the widespread presence of modern robots in various sectors such as industry, logistics, service, and healthcare. These robots will possess local as well as network computing and communication capabilities that enable them to operate in diverse environments while gathering and exchanging information. AI will be an inherent component of these robots, empowering them to obtain new skills and disseminate their acquired knowledge across different systems. The more these intelligent robotic agents integrate synergistically into varied environments, the more they will take over diverse tasks while actively cooperating with humans. Naturally, many challenges will emerge as AI and robotics become ubiquitous. Precisely because of this ubiquity, the challenge presented by their energy demand deserves particular attention.
% ---
\begin{figure*}[t!]
	\centering
	\hspace*{\fill}
	\begin{subfigure}[t]{0.45\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{cai_concept.png} \label{fig:cai}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.45\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{eai_concept.png} \label{fig:eai}
	\end{subfigure}	
	\hspace*{\fill}
%	\\
%	\hspace*{\fill}
%	\begin{subfigure}[t]{0.95\textwidth}
%		\subcaption{}
%		\includegraphics[width=\textwidth]{embodied_ai_learning_pipeline.png} \label{fig:embodied_ai_pipeline}
%	\end{subfigure}	
%	\hspace*{\fill}
	\caption[] {\label{fig:cai_and_eai_general} \textbf{Classical and embodied AI.} Differences in learning and deployment in (\subref{fig:cai}) CAI and (\subref{fig:eai})  EAI. %(\subref{fig:embodied_ai_pipeline}) Standard skill execution pipeline of isolated EAI agents.
	}
\end{figure*}
% ---

%\sout{Aside from the demands associated to passive data collection (i.e., the CAI agent is not directly involved in the generation/collection of the data) via several edge devices, learning in CAI craves a considerable energy quota to process the data and to train, validate, and test several models.}
%To address the energy demands from AI and robotics we distinguish between \textit{classical AI} (CAI) and \textit{embodied AI} (EAI). We consider the former as the set of algorithms that address purely computational problems; that is, problems that are decoupled from physical systems and do not have interaction with the physical world, see Fig.~\ref{fig:cai_and_eai}. In CAI, data collection occurs passively via several edge devices. As such, a prototypical CAI agent is not directly involved in the generation or collection of training data. In general, and aside from the energy associated to passive data collection and processing, the energetic demand of a CAI application can be subdivided in the energy for learning---i.e., for training the models---and deployment---to run inference and prediction\footnote{A third relevant and often neglected area of CAI energy consumption concerns manufacturing the required computation and communication hardware.}. When a given CAI application targets different tasks or even systems, it is expected that both model and training data carry enough information to make knowledge transfer possible and boost learning. However, retraining is required---sometimes even from tabula rasa---when this information is absent, resulting in highly energy-inefficient learning. Furthermore, even if learning occurs only once, deployment alone can demand considerable energy due to the constant execution of the model. Depending on the application, it is possible that the energetic cost of CAI sometimes outweighs the benefits \cite{Strubell2019EnergyPolicyConsiderations}. Even current breakthroughs like the use of transformer models for Natural Language Processing come with their energetic challenges \cite{Cao2020TowardsAccurateReliable}.
To address the energy demands of AI and robotics, we differentiate between \textit{classical AI} (CAI) and \textit{embodied AI} (EAI), as illustrated in Fig.~\ref{fig:cai_and_eai_general}. We consider CAI as the set of methods and algorithms that tackle purely computational problems, detached from embodied systems and lacking interaction with the physical world (see Fig.~\ref{fig:cai}). In CAI, data collection occurs passively through various edge devices, with a prototypical CAI agent not directly involved in generating or collecting training data. The energetic demands of CAI applications primarily stem from learning, i.e., training the models, and deployment, i.e., running inference and prediction \cite{Vries2023growingenergyfootprint}. Additionally, the manufacturing of the required computation and communication hardware for CAI is often an overlooked source of energy consumption. 

For CAI applications targeting diverse tasks or systems, successful knowledge transfer relies on the adequacy of the learning paradigm and both model and training data carrying enough information about the problem. However, in the absence of any of these factors, retraining, sometimes from scratch, becomes necessary, leading to highly energy-inefficient learning processes. Even if learning occurs only once, the ongoing deployment of the model can demand significant energy due to constant computationally intensive execution \cite{Vries2023growingenergyfootprint}. Thus, depending on the application, the energetic cost of learning and deployment in CAI can outweigh the benefits \cite{Strubell2019EnergyPolicyConsiderations}. This is also applicable to recent breakthroughs, such as transformer models for Natural Language Processing, whose results also come accompanied by energetic challenges \cite{Cao2020TowardsAccurateReliable}.

%With the evolution towards EAI, i.e., the integration of AI and robotics \cite{Pfeifer2004Embodiedartificialintelligence}, the energy usage spectrum broadens. Since the real world cannot be faithfully replicated in virtual environments, and despite the considerable advances in sim-to-real applications \cite{Chebotar2019Closingsimreal}, learning and deployment in EAI necessitates constant energy-expending interaction with the physical environment for active data generation. As shown in Fig.~\ref{fig:eai}, this is achieved through physical agents; e.g., robots, vehicles, drones, etc. Learning and mastering a skill in the physical world implies its constant and repeated execution, expending energy on motion and interaction during each run. Consider, for example, autonomous driving, where the vehicle is a rather rudimentary form of an EAI agent operating primarily in a structured human-made environment. Apart from consuming energy by simply fulfilling its purpose (i.e., autonomous movement), the vehicle consumes additional energy on motion to collect the necessary data to retrain and improve the policy model. Another, perhaps inconspicuous, example is that of household robots. In their way to automate a high percentage of domestic chores in the chores term \cite{Lehdonvirta2022futuresunpaidwork}, they will undergo constant retraining due to the inconspicuous changing dynamics of household environments. 
The evolution towards EAI, the integration of AI and robotics \cite{Pfeifer2004Embodiedartificialintelligence}, expands the energy usage spectrum. Unlike virtual environments, the real world cannot be faithfully replicated, despite considerable advances in sim-to-real applications \cite{Chebotar2019Closingsimreal}. Learning and deployment in EAI demand constant energy-expending interaction with the physical environment for active data generation, as depicted in Fig.~\ref{fig:eai}, facilitated by physical agents like robots, vehicles, and drones. Mastering skills in the physical realm requires continuous and repeated execution, consuming energy for motion and interaction in each instance. Take autonomous driving, for example, where vehicles function as rudimentary EAI agents in structured human-made environments. Besides energy for autonomous movement, vehicles expend additional energy on motion to collect data necessary for retraining and improving the policy model. Another example is household robots, aiming to automate a high percentage of domestic chores \cite{Lehdonvirta2022futuresunpaidwork}. They undergo constant retraining due to the subtle and changing dynamics of household environments.
% ---
\begin{figure*}[t!]
	\centering
	\hspace*{\fill}
	\includegraphics[width=0.95\textwidth]{embodied_ai_learning_pipeline.png}
	\hspace*{\fill}
	\caption[] {\label{fig:embodied_ai_pipeline} \textbf{Standard skill execution pipeline of isolated EAI agents.} {Three fundamental energy expenditure categories are identified during the learning or execution of a skill by an EAI agent.}}
\end{figure*}
% ---

Unlike the standard standard energy for learning and deployment classification in CAI, the analysis of the energetic requirements in EAI requires an alternative perspective. A closer look at the standard skill execution pipeline of a prototypical EAI agent (Fig.~\ref{fig:embodied_ai_pipeline}) allows the identification of three fundamental energetic expenditure categories, namely:
% ---
\begin{enumerate}
	\item Computation and Communication Expenditure (CCE): Coincident with CAI, it refers to the energy used by the computation and communication processes required by planning, querying, exploration, and training routines.
	\item Basal Energy Expenditure (BEE): This body-related energy is associated with the execution of basic functions of the EAI agent. For example, operating energy, gravity compensation, and proprioceptive intelligence algorithms in robots, hovering in drones, running on-board system standby in autonomous vehicles, etc.
	\item Motion and Interaction Expenditure (MIE): Defines the energy spent on physical interactions, namely, executing a particular skill in a certain form. For example, taking an object from an initial to a target location within a given time following a particular trajectory.
\end{enumerate}
% ---

An important fact in EAI is the existence of a lower bound to the energy required to carry out a skill that is independent from the agent. Consider a generic skill $\tau$---such as a pick-and-place operation---and suppose the optimal trajectory $p^\star$ for moving an object from its origin to its destination is known. The intrinsic properties of the object and the optimal path $p^\star$ uniquely define the minimum energy requirement $E^\star_{\tau}$ needed to perform skill $\tau$. The implication is that the total energy expended by any agent in the process of mastering or executing a skill is higher than $E^\star_{\tau}$ as a result of the required computational ($E_{CCE}$), body-related ($E_{BEE}$), and physical interaction ($E_{MIE}$) energy expenditures; i.e.,
% ---
\begin{equation}\label{eq:skill_energy_in_eai}
	E_{\tau} =  \underbrace{E_{BEE}}_{\text{Body-dependent energy}} + \underbrace{E_{CCE} + E_{MIE}}_{\text{Learning energy}} \gg \underbrace{E^\star_{\tau}}_{\text{Skill energy}} .
\end{equation}
% ---
Notice that if Eq.~\eqref{eq:skill_energy_in_eai} was used to describe the energy consumption of a task in CAI, $E_{BEE}$ could be associated with the edge devices and $E_{CCE}$ would the main source of energy consumption. Crucially, in CAI the expenditures $E^\star_{\tau}$ and $E_{MIE}$ do not exist since physical interaction is absent.

\redtext{This work discusses how current learning paradigms in artificial intelligence, coupled with the anticipated exponential growth in the number of operational robot units, present energetic challenges due to inefficient knowledge utilization. We delve into these challenges and argue for the adoption of a learning paradigm that explicitly leverages interconnection and knowledge sharing among EAI agents. Consequently, we propose collective learning as the optimal paradigm to facilitate faster and more efficient learning, thereby mitigating the energetic challenges in EAI. Specifically, we examine the ideal dynamics of knowledge sharing that a group of robots needs to exhibit to effectively realize the benefits of a collective learning strategy.}


% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
%\section*{Energy grand challenges in AI}\label{sec:energy_grand_challenges}
\section*{The energy landscape in AI}\label{sec:energy_grand_challenges}
Only recently, have the energetic requirements of both CAI and EAI gained attention within the AI and robotics research communities. Grasping its implications, however, necessitates recognizing the challenges posed by the substantial escalation of the energy expenditure categories associated to the CCE, BEE, and MIE energy expenditures in relation to the rapid proliferation of CAI and EAI applications. Of vital importance is underscoring the negative impact of persistently employing learning paradigms that do not actively foster and systematically leverage knowledge exchange among agents.
% ---
\begin{figure*}[!t]
	\centering
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{data_center_energy_consumption.png} \label{fig:dataCenterEnergy}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{ir_energy_projections.png} \label{fig:ir_energy}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{cb_energy_projections.png} \label{fig:cobot_energy}
	\end{subfigure}	
	\hspace*{\fill}
	\caption[] {\label{fig:energy_demands_AI_robotics} \textbf{Energy demands in AI and robotics.} \subref{fig:dataCenterEnergy} Global electricity demand of data centers, adapted from \cite{andrae2015global}. The estimated World Robot Energy Consumption of \subref{fig:ir_energy} industrial robots and \subref{fig:cobot_energy} collaborative robots.}
\end{figure*}
% ---

% SUBSECTION ========================================================================================
\paragraph*{\textbf{Challenge 1} (C1): Energy for AI infrastructure}
The remarkable progress witnessed across various domains, attributed to the exponential growth of AI applications, comes at a significant cost. These advancements require substantial computational power for cutting-edge machine learning algorithms to process, analyze, and learn from extensive data. This often necessitates numerous iterations to converge \cite{Strubell2019EnergyPolicyConsiderations}. Researchers and corporations heavily depend on existing infrastructure or cloud computing services in data centers for the energy-intensive computational workloads during the learning and deployment phases. Consequently, there has been a clear spike in energy consumption in data centers and associated hardware, such as Graphics Processing Units (GPUs). Training AI models in data centers is estimated to demand about three times more energy than traditional cloud tasks, underlining the strain on resources \cite{Thomas2023cloudusesmassive}.

Consider, for instance, the latest breakthroughs ushered in by generative AI, including large language models (LLM) and text-to-image models. These models boast billions of parameters and necessitate thousands of deep learning GPU units and millions of GPU hours for training \cite{Vanian2023ChatGPTgenerativeAI, Corbyn2023Nvidiachipmaker}. As more AI applications are developed, the demand for AI infrastructure is surging, leading to a substantial increase in GPU-based AI servers being sold to meet this demand. Naturally, this escalation in demand translates to a parallel rise in data center energy consumption. Globally, data center energy consumption surged from 200 TWh in 2015 to an estimated 220-320 TWh in 2021, according to data from the International Energy Agency \footnote{Data from the International Energy Agency, available at \url{https://www.iea.org/reports/data-centres-and-data-transmission-networks}}. This concerning trend is visually depicted in Figure~\ref{fig:dataCenterEnergy}.

This escalating energy usage has sparked growing concerns within the research community regarding the adverse environmental effects of AI and machine learning. Recent discussions on this matter, such as \cite{schwartz2019green}, \cite{vinuesa2020role},  \cite{Strubell2019EnergyPolicyConsiderations}, and \cite{Luccioni2023EstimatingCarbonFootprint}, delve into the efficiency of computation-intensive deep learning algorithms. Various metrics have been established to gauge the energy consumption of machine learning algorithms. These include assessing energy efficiency during development phases \cite{zhou2020hulk}, analyzing accuracy, model size, time, and CPU/GPU energy consumption for training and inference phases \cite{Dalgren2019GreenMLA}, as well as encompassing other system-level performance indicators like real-time metrics, instruction-level analysis, and hardware-level power estimation \cite{garcia2019estimation}. Recent works focused on LLMs discussing various aspects such as hardware efficiency, model architectures and algorithms in relation to energy consumption \cite{Vries2023growingenergyfootprint} and provide comparisons including their power consumption and CO$_2$ emissions \cite{SIHCAI2023ArtificialIntelligenceIndex}. Despite the burgeoning awareness surrounding AI's energy consumption, tangible actions remain scarce in addressing the underlying issues and proposing potential remedies.

It is worth noting that, unlike state-of-the-art machine learning models (e.g., transformer models) that are mostly trained once on a large amount of data, EAI systems will use models that require constant retraining and re-evaluation, such as neural architecture search \cite{real2019regularized}.

% SUBSECTION ========================================================================================
\paragraph*{\textbf{Challenge 2} (C2): The escalating energy demand of a robotic revolution}\label{sec:robots_challenge}
The continuous growth in the number of robots in operation is a notable trend amplified by the rise of Industry 4.0 and the implementation of smart factories, alongside the expanding utilization of robots in various service-oriented applications. Despite the advancements in robot technology that have yielded improved energy efficiency, the predominant focus remains on individual systems, often disregarding the collective impact of all active units.

Over the past decade, the installation base of industrial robots has undergone a remarkable transformation, escalating from 1.2 million units in 2012 to approximately 3.5 million units in 2022, an astonishing surge constituting a 290 \% increase. According to data from the International Federation of Robotics (IFR), the annual growth rate within this time frame has consistently ranged between 12 \% and 15 \% \cite{IFR2019}. Extrapolation of this growth rate suggests that in the coming years, four million robots will be operational within factories across the globe. This trend is depicted in Fig.\ref{fig:ir_stock}\footnote{These projections closely align with the slightly more cautious estimates presented by \textit{The Boston Consulting Group} in \cite{sirkin2015}.}. Using the estimated install base and under the assumption of round-the-clock operation, we can approximate the forthcoming energy demand attributable to industrial robots---termed the \textit{World Robot Energy Consumption} (WREC), shown in Fig.~\ref{fig:ir_energy}. A description of how we arrived at these estimates is provided in the \nameref{sec:supplementary_materials} Sec.~\ref{sec:app_robot_ener_consumption}. To contextualize the WREC, in 2025 it constitutes 7.2 \% of Germany's installed electricity generation capacity \cite{fraunhofer2016}.

The far-reaching influence of collaborative and service robots echoes the significance observed among their industrial counterparts. Collaborative robots (cobots), for instance, have undergone a paradigm shift, progressing from accounting for a mere 6 \% of the market in 2017 to constituting around one-quarter of annual installations \cite{tobe2015}, as illustrated in Fig.~\ref{fig:industrial_cobot_share}. Drawing from analogous assumptions applied to industrial robots (see Sec.~\ref{sec:app_cobot_ener_consumption}), Fig.~\ref{fig:cobot_stock} and \ref{fig:cobot_energy} lay out the projected growth trajectory and the corresponding energy consumption within this robot category. Concurrently, the domain of service robots is experiencing an analogous surge. For instance, the International Federation of Robotics estimated sales of approximately 35 million privately used service robots in 2018 \cite{IFR2015}. These robots find utility across various fields, including logistics, defense, public relations, medical applications, and beyond, underlining their alignment with the escalating trends observed among industrial and collaborative robots.

% SUBSECTION ========================================================================================
\paragraph*{\textbf{Challenge 3} (C3): Energy for manufacturing}
This final challenge broadens the perspective, encompassing the energetic expenditure associated with manufacturing the hardware required for CAI and EAI applications. While not our primary focus, we touch upon this aspect for comprehensiveness and to encourage its exploration in forthcoming research. C3 entails two primary facets. First, it involves the energy outlay for procuring the materials for robot manufacturing and the associated computational hardware (e.g., processors, GPUs, and AI servers). Second, it pertains to the energy consumption intrinsic to the manufacturing process. Given the direct correlation between energy demand and the number of CAI applications and EAI agents produced, an exponential rise in the latter directly corresponds to escalated energy consumption for their production. The assessment and formulation of strategies to address this aspect constitute the crux of this challenge. While an immediate solution may not be evident, and since substantial energy savings in raw material procurement may be impractical, significant potential lies on the recycling of electronic components of computer and robot hardware as a means of conserving energy \footnote{An example of such an endeavor is the international competition  \textit{Robothon\textsuperscript{\textregistered} - The Grand Challenge} where hardware and software are developed to autonomously disassemble and sort electronic waste, see~\url{https://automatica-munich.com/en/munich-i/robothon/} .}. Notably, the prospect of future EAI agents autonomously fabricating other EAI agents intertwines this challenge directly with the realms of challenges C1 and C2.

% SUBSECTION ========================================================================================
\paragraph*{\textbf{Tackling the challenges}}
One idea seems to lurk behind the three challenges: \emph{\say{more AI applications, more energy demand}}. Indeed, the trends in Fig.~\ref{fig:energy_demands_AI_robotics} suggest that the CCE, BEE, and MIE expenditures will follow a similar pattern. From the EAI perspective, ongoing efforts to minimize BEE and improve the MIE include works like \cite{schroder2014, chalmers2015, mohammed2014, chemnitz2011}; which advocate strategies such as elastic actuation and optimized hardware selection and storage, energy sharing, and motion planning. As for CCE, it is important to design better hardware for more efficient parallel computing and to decentralize the computation leveraging the local processing capabilities of edge devices and robots. Perhaps even more relevant is to define sample-efficient learning algorithms with optimized models that account for the recurrent inference and prediction processes in CAI and for the constant learning need in EAI agents. However, we believe that achieving greater energy efficiency in AI requires a broader perspective than just enhancing hardware and optimizing the individual agents' learning strategies. The actual key to a significant breakthrough lies in tapping into the vast reservoir of knowledge accumulated by AI agents. 

In particular, true knowledge exchange can alleviate the burdens associated with the CCE and MIE expenditures in embodied AI. By promoting and leveraging concurrent knowledge exchange, it becomes possible to reduce the computational and mechanical energy required to acquire new skills. To explore this uncharted terrain, we propose considering the collective learning paradigm introduced in \cite{Haddadin2014SystemzumErstellen,Haddadin2015Systemgeneratingsets} as a means to harness scalability and facilitate knowledge exchange, thereby enhancing energy efficiency in the domain of EAI.


% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Modeling the dynamics of skill knowledge}\label{sec:knowledge_dynamics_model}
Understanding the energy and time demands represented by a team of $m$ robots learning a universe $\mathcal{S}=\left\lbrace s_1,s_2,\ldots s_j,\ldots, s_{N_\mathcal{S}}\right\rbrace$ of skills, with $|\mathcal{S}| = N_\mathcal{S}$, requires looking at how knowledge about a skill is gained and what effect it can have on the acquisition of any new skill knowledge. 

To start, we consider that the \emph{complexity} $c_j$ of a skill $ s_j $ is the number of trial episodes $n$ needed to successfully learn the skill, i.e., all actions and states visited by an EAI agent until a stopping criterion is reached. Additionally, %where the system resembles the following behavior.
% ---
% \begin{tcolorbox}
% 	\begin{assumption}\label{assumption:time}
% 		Given a large enough number of robots performing a large number of skills, on average, the power $P_0$ required by any given robot during learning and the time $\Delta t$ allocated to the execution of every trial episode $n$ are approximately constant, see Fig.~\ref{fig:power_per_episode}.
% 	\end{assumption}
% \end{tcolorbox}
\begin{tcolorbox}
	\begin{assumption}\label{assumption:power_and_episode_time}
		the average behavior of a system where both $m$ and $N_\mathcal{S}$ are large can be described by the power $P_0$ required by any agent during learning and the mean execution time $\Delta t$ of every trial episode $n$, with both approximately constant; see Fig.~\ref{fig:power_per_episode}.
	\end{assumption}
\end{tcolorbox}
% ---
\noindent As a consequence of Asm.~\ref{assumption:power_and_episode_time}, and according to Eqs.~\eqref{eq:energy_per_episode},\eqref{eq:energy_per_skill}, and \eqref{eq:total_energy} in Sec.~\ref{sec:power_per_episode}, the energy demand of an EAI agent learning a skill (or set of skills) is proportional to the skill(s) complexity.

% ===================================================================================================
\paragraph*{Similarity and knowledge}
Let $\mathcal{Z}_k \subset \mathcal{S}$ be a subset of $N_{\mathcal{Z}_k}$ skills that share high similarity; i.e., a \emph{cluster} of similar skills, see Fig.~\ref{fig:skill_similarity}. Furthermore, consider a second set $\mathcal{\zeta}_k \subset \mathcal{Z}_k$ that denotes already learned skills from $\mathcal{Z}_k$. Furthermore, the following assumption is made.  
%---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:skill_clustering} If the similarity among a set of skills is significant, exchanging acquired knowledge from these skills expedites the overall learning process.
	\end{assumption}
\end{tcolorbox}
% ---
\noindent This implies that the $j$-th skill in the $k$-th cluster $s_{j,k} \in \mathcal{Z}_k$ can always benefit from the knowledge contained in $\mathcal{\zeta}_k$. Consequently, the more skills in $\mathcal{\zeta}_k$, the less knowledge about $ s_{j,k} $ remains to be learned. To model this effect, we introduce a function $\bar{\sigma}_{j,k}\left(n\right)\in [0,1]$ that expresses the knowledge about a skill $s_{j,k} \in \mathcal{Z}_k \setminus \mathcal{\zeta}_k$ that \emph{is not} contained in the knowledge base of $\mathcal{\zeta}_k$. The function $\bar{\sigma}_{j,k}(\cdot)$ satisfies
% ---
\begin{equation}\label{eq:sigma_bar_conditions}
	\bar{\sigma}_{j,k}\left(n\right) = 
	\begin{cases}
		1 & \text{$\mathcal{\zeta}_k=\emptyset$},\\
		0 &\text{$\mathcal{\zeta}_k$ has \emph{all} knowledge of $s_{j,k}$}.
	\end{cases}
\end{equation}
% ---
Conceptually, $\bar{\sigma}_ {j,k}\left(\cdot\right)$ is the fraction of knowledge from ${\mathcal{Z}_k}$ that remains to be learned.
% ---
\begin{figure*}[!t]
	\centering
	\hspace*{\fill}
	\begin{subfigure}[t]{0.45\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{skill_similarity.png} \label{fig:skill_similarity}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{knowledge_idealization.png} \label{fig:knowledge_idealization}
	\end{subfigure}
	\hspace*{\fill}
	\caption[] {\label{fig:experimental_results} \textbf{Skill similarity and knowledge.} \subref{fig:skill_similarity} Similar skills in $\mathcal{S}$ can be grouped into clusters $\mathcal{Z}_k$, \subref{fig:knowledge_idealization} remaining knowledge to learn a new skill $s_{j,k}$.}	
\end{figure*}
% ---
% ===================================================================================================
\paragraph*{Leveraging the acquired knowledge}
To evaluate the effect of knowledge exchange during learning on the complexity of mastering a skill, we introduce an upper bound called the skill \textit{fundamental complexity} $c_0$, which describes the maximum number of trial episodes required to learn \emph{any} skill. If, in learning a skill $ s_{j,k} $, an EAI agent can access and use the knowledge contained in $\mathcal{\zeta}_k$; then, two effects take place:
% ---
\begin{enumerate}
	\item There is less remaining knowledge, reflected in the initial value; i.e. $\bar{\sigma}_{j,k}(0) < 1$
	\item The knowledge acquisition rate increases
\end{enumerate}
% ---
%associated complexity $ c_{j,k} $ is necessarily smaller than the fundamental complexity $c_{0}$; i.e. $c_{j,k} < c_0~\forall j>1$.
These effects signify that the remaining knowledge scales down as a function of the number of learned skills $N_{\zeta_k}=|\mathcal{\zeta}_k|$. As a consequence, the complexity $c_{j,k}$ of said skill is smaller than the fundamental complexity $c_0$. Additionally, without loss of generality, under knowledge exchange we can consider that
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:exponential_decrease} the remaining knowledge function $\bar{\sigma}_{j,k}(\cdot)$ has a monotonically decreasing behavior.
	\end{assumption}
\end{tcolorbox} 
% ---
\noindent An idealization of the behavior satisfying Asm.~\ref{assumption:exponential_decrease} and Eq.~\eqref{eq:sigma_bar_conditions} can be modeled via a differential equation depending on the trial episodes $n$ and is parameterized by the number of already learned skills $N_{\zeta_k}$. As such,
% ---
\begin{definition}\label{assumption:ode_model} the remaining knowledge function $\bar{\sigma}_{j,k}$ is modeled as the first order dynamical system
	\begin{subequations}\label{eq:simple_knowledge_dynamics}
		\begin{empheq}[left=\empheqlbrace]{align}
			\dot{\bar{\sigma}}_{j,k}\left(n\right) &  = -f_{j,k} \left(N_{\zeta_k} \right) \bar{\sigma}_{j,k}\left(n\right),\\
			\bar{\sigma}_{j,k}(0) &  =  g_{j,k} \left(N_{\zeta_k}\right).
		\end{empheq}
	\end{subequations}
\end{definition}
% ---
\noindent Its solution
% ---
\begin{equation}\label{eq:knowledge_exponential_form}
	\bar{\sigma}_{j,k}(n) = g_{j,k}\left(N_{\zeta_k}\right) e ^{-f_{j,k}\left(N_{\zeta_k}\right) n} \in (0,1],
\end{equation}
% ---
exhibits the desired behavior, shown in Fig.~\ref{fig:knowledge_idealization}. The function $f_{j,k}\left(N_{\zeta_k}\right)$ models one of the effects resulting from the exploitation of the knowledge available in $\zeta_k$, namely, the increase of the learning rate. The second effect, i.e. the reduction in the initial remaining knowledge $\bar{\sigma}_{j,k}(0)$ is controlled by the term $g_{j,k}\left(N_{\zeta_k}\right)$, which is also dependent on the number of learned skills. The learning threshold $\epsilon$ in Fig.~\ref{fig:knowledge_idealization} indicates when the remaining knowledge is negligible and $s_{j,k}$ is considered as learned.
% ---
%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=0.45\textwidth]{fig/knowledge_idealization.png}
%	\caption{Remaining knowledge to learn a new skill $s_{j,k}$.}
%	\label{fig:knowledge_idealization}
%\end{figure}
% ---

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Knowledge sharing under different learning paradigms}
% ---
\begin{figure*}[!t]
	\centering
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{intra_skill_learning.png} \label{fig:intra_skill_learning}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{cluster_to_cluster_knowledge_transfer_parallel.png} \label{fig:cluster_to_cluster_knowledge_transfer_parallel}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.32\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{cl_example_figure.png} \label{fig:cl_example_figure}
	\end{subfigure}	
	\hspace*{\fill}
	\caption[] {\label{fig:learning_paradigms_conceptual_figure} \textbf{The different learning paradigms.} \subref{fig:intra_skill_learning} Incremental learning benefits from the significant similarity of skills belonging to the same cluster. \subref{fig:cluster_to_cluster_knowledge_transfer_parallel} In transfer learning knowledge is shared from different origin clusters to the target cluster, notice that using many robots (e.g. two robots $r_1$ and $r_2$) without inter-agent knowledge exchange among them only subdivides the problem. \subref{fig:cl_example_figure} Exchange of knowledge between EAI agents enables collective learning.}
\end{figure*}
% ---

For the upcoming analysis we consider an idealized reference system in which a large number of robots coexist learning a large number of skills. Such system exhibits
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:average_behavior}
		an average behavior that results from comparable EAI agents learning and executing the skills in $\mathcal{S}$ ordered and segregated according to their similarity.
	\end{assumption}
\end{tcolorbox}
%---
\noindent Each of the EAI agents in the system
\begin{tcolorbox}
	\begin{assumption}\label{assumption:agent_similarity}
		has the same capabilities with highly similar BEE and MIE expenditures.
	\end{assumption}
\end{tcolorbox}
%---
\noindent The large number of skills in $\mathcal{S}$ implies that
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:cluster_size}
		every cluster $\mathcal{Z}_{k}$ contains the same number $N_{\mathcal{Z}} $ of skills.
	\end{assumption}
\end{tcolorbox}
% ---
\noindent By virtue of the optimal ordering of the skills and the balanced size of the clusters,
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:cluster_transferability}
		the knowledge transferability between in-cluster skills ---modeled by \eqref{eq:f_function_incremental} and \eqref{eq:g_function_incremental}--- is assumed to be equal; as is transferability between clusters, see \eqref{eq:f_function_transfer} and \eqref{eq:g_function_transfer}.
	\end{assumption}
\end{tcolorbox}
% ---
\noindent Finally, the different learning paradigms that exploit the collected knowledge by the EAI agents rely on the fact that
% ---
\begin{tcolorbox}
	\begin{assumption}\label{assumption:enabling_agorithms}
		there are advanced control and machine learning algorithms designed to inherently use this knowledge.
	\end{assumption}
\end{tcolorbox}
% ---

% ===================================================================================================
\paragraph*{Conventional learning paradigms} 
%Now we briefly go over the fundamental aspects of the different learning paradigms considered for EAI, a detailed discussion is provided in \nameref{sec:materials_and_methods}. 
When an EAI agent performs \textbf{isolated learning} (IsL), it learns each new skill from the ground up, disregarding the accumulating knowledge from already learned skills. In contrast, \textbf{incremental learning} (IL) corresponds to the case where an agent benefits from the continuous aggregation and exchange of knowledge from \emph{intra-cluster} skills in virtue of their significant similarity. As depicted in Fig.~\ref{fig:intra_skill_learning}, a robot ($r_1$ in this case) learns every skill in $\mathcal{Z}_1$ with a rate $\alpha$ ---the self-loops --- but also retains and uses the acquired knowledge to learn subsequent skills. \textbf{Transfer learning} (TL) alone refers to the one-time \emph{inter-cluster} exchange of knowledge. TL represents the exchange of knowledge from the skills learned in different \emph{origin} clusters $\mathcal{O} = \{ \mathcal{Z}_1,\mathcal{Z}_2,\ldots,\mathcal{Z}_{k-1} \}$ to the skills that will be learned in a \emph{destination} cluster $\mathcal{Z}_k$ (see Fig.~\ref{fig:cluster_to_cluster_knowledge_transfer_parallel}). Concretely, the effect that TL has on the skills of the destination cluster is the reduction of the initial remaining knowledge and the increase of the initial learning rate for all the skills in the $k$-th cluster via the parameter $\beta_k$. In general, transfer learning is always complemented with IL, and so we define the combination of both \textbf{transfer with incremental learning} (TIL) as the third learning paradigm.  %In all these paradigms, there is no exchange of knowledge among agents, which implies that employing $m$ agents in parallel to learn $m$ skills only subdivides the problem and does not provide an advantage. 
A detailed discussion on the effects that these paradigms have on the skill complexity is provided in Sec.~\ref{sec:materials_and_methods}.

% ===================================================================================================
\paragraph*{\textbf{Collective learning (CL)}}
This paradigm  goes beyond simple parallelization. In CL $m$ robotic agents $ \left\lbrace r_i \right\rbrace_{i=1}^{m} $  develop and accumulate a common mind (body of knowledge) dynamically via networked interactions where individual experience, knowledge, and skills are disseminated to all the other elements in the collective. Information flows vertically as previous knowledge is passed on and horizontally by sharing concurrent experience between agents. Knowledge can be replicated, complemented, and further developed via these mechanisms. We take from \cite{Garavan2012CollectiveLearning} two notions central in CL that apply to EAI agents:
$(I)$ capability to restructure and meet changing conditions, and $(II)$ aggregation of skills, knowledge, and behaviors. Moreover, to enable CL, it is assumed that an inter-agent communication protocol and the appropriate infrastructure are in place that enables agents to concurrently exchange and integrate the self-acquired and received knowledge to incrementally speed up the learning of all the agents as a whole. As a result, intra- and inter-cluster knowledge transfer is possible. Naturally, the CL paradigm involves a complex scheduling problem to determine the optimal skill distribution and inter-agent knowledge-sharing strategy. 

One could argue that recent contributions have partially addressed aspects of the CL paradigm, as exemplified by works such as \cite{levine2018learning, rudin2022learning, flairop2023}. However, it is essential to clarify the focus of this article: we do not delve into the specifics of algorithms required for CL, as many of them are, despite vast progress in machine learning still nonexistent or under development, nor do we explore the necessary advancements in processing and communication infrastructure to enable CL. Instead, our primary objective is to illustrate the overarching systemic behavior inherent in the CL paradigm, grounded on Assumptions~\ref{assumption:average_behavior}, \ref{assumption:agent_similarity},~\ref{assumption:cluster_size}, and~\ref{assumption:cluster_transferability}; particularly,  regarding its target knowledge-sharing dynamics.

Fig.~\ref{fig:cl_example_figure} illustrates the CL concept, where the self-loop represents the dynamics of a single robot learning at a rate $\alpha$. The exchange of knowledge across agents is represented via the cross-couplings weighted by a parameter $\gamma$ that models how efficient is the bidirectional pairwise knowledge exchange. Similar to TL, if two robots exchange knowledge about skills with low similarity, i.e., skills in different clusters, then $\gamma$ is scaled by the inter-cluster transferability parameter $\beta$. In CL the dynamics of the remaining knowledge is described by
% ---
\begin{subequations}\label{eq:collective_knowledge_dynamics}
	\begin{empheq}[left=\empheqlbrace]{align}
		\dot{\bar{\bm{\sigma}}}^{(CL)}_{j,k}\left(n\right) &= \left[  h_{j,k}\left(N_{\zeta_k},r\right) \bm{I} + \gamma \bm{A} \odot \bm{B}  \right] \bar{\bm{\sigma}}^{(CL)}_{j,k}\left(n\right)\\
		\bar{\bm{\sigma}}^{(CL)}_{j,k}(0) &= g_{j,k}\left( N_{\zeta_k}, r\right) \bm{I},
	\end{empheq}
\end{subequations}
% ---
where $r=m$ is the number of robots that exchange knowledge among them. Now, $\bar{\bm{\sigma}}^{}_{j,k} \in \mathbb{R}^r$ is a vector that represents the dynamics of the remaining knowledge of all the $m$ skills being concurrently learned. $\bm{A} \in \mathbb{R}^{r \times r}$ is a zero-diagonal symmetric adjacency matrix whose entry $(\bm{A})_{i,j} = 1$ if robot $r_i$ exchanges knowledge with robot $r_j$ and $(\bm{A})_{i,j} = 0$ if it does not. The term $\gamma \in \mathbb{R}_+ $ weighs the knowledge exchange strength among robots. Since there may be robots learning skills in different clusters at the same time, the matrix $\bm{B}$, whose entries are $\left(\bm{B}\right)_{i,j} \in \left \lbrace 1, \beta_{k} \right \rbrace$, with
% ---
\begin{equation}
	%\beta_{k} = 1/N_\mathcal{K}, 
	\beta_{k} = r\frac{ N_{\zeta_k}}{N_\mathcal{S}}, 
\end{equation}
% ---
scales down the knowledge contributions between robots from different clusters. Finally, the operator $\odot$ represents the Hadamard product of matrices. The functions $ h(\cdot)$ and $g(\cdot)$ in Eq.~\eqref{eq:collective_knowledge_dynamics}, with the former defined as
% ---
\begin{equation}\label{eq:f_function_collective}
	h_{j,k}\left(N_{\zeta_k},r\right) = -\alpha \left( \frac{\eta r N_{\zeta_k} + 1}{1 - \beta_k} \right),
\end{equation}
% --- 
are dependent on the number of knowledge-exchanging robots, which directly impacts the number of skills that enter $\zeta_k$ after a learning cycle.

The dynamics of the remaining knowledge for the considered learning paradigms are described by replacing in Eq.~\eqref{eq:simple_knowledge_dynamics} the expressions for the rate of remaining knowledge and the corresponding initial value as per Table~\ref{tab:learning_paradigms_expressions}; where
% ---
\begin{enumerate}
	\item the constant $ \alpha>0$ models the rate at which a robot in isolation learns any given skill,
	\item the constant $\eta>0$ represents the efficiency of knowledge exchange from $\zeta_k$ to $s_{j,k}$,
	\item the factor $\delta>0$ controls the rate of exponential decrease in the initial remaining knowledge value, and
	\item $\beta_k$ is the head start granted by knowledge transfer from other clusters to the skills in $\mathcal{Z}_k$.
\end{enumerate}
% ---
%\begin{table}[!ht]
%	\caption{The dynamics of classical learning paradigms.\label{tab:learning_paradigms_expressions}}
%	\begin{center}
%		\begin{adjustbox}{width=\textwidth}
%			\begin{tabular}{ |c|c|c|c|c|} 
%				\hline
%				Learning type & IsL & IL & TIL & CL  \\
%				\hline
%				Rate $f_{j,k}\left(\cdot \right)$ & $ \alpha$ & $ \alpha\left(\eta N_{\zeta_k} + 1 \right)$ & $\alpha \left( \frac{\eta N_{\zeta_k} + 1}{1 - \beta_k} \right)$ & $h_{j,k}\left(N_{\zeta_k},r\right) \bm{I} + \gamma \bm{A} \odot \bm{B}$ \\[5ex]
%				\hline
%				Initial condition $g_{j,k}\left(\cdot \right)$  & $1$ & $e^{-\delta N_{\zeta_k}}$ & $(1-\beta_k) e^{-\delta N_{\zeta_k}}$ & $(1-\beta_k) e^{-\delta r N_{\zeta_k}} \bm{I}$\\[5ex]
%				\hline
%			\end{tabular}
%		\end{adjustbox}
%	\end{center}	
%\end{table}
% ---

\begin{table}[!ht]
\caption{The dynamics of the learning paradigms.\label{tab:learning_paradigms_expressions}}
\begin{center}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{|l||*{4}{c|}}\hline
	Learning type
	&\makebox[3em]{IsL}&\makebox[3em]{IL}&\makebox[3em]{TIL}
	&\makebox[3em]{CL}\\\hline\hline
	Rate $f_{j,k}\left(\cdot \right)$  &$ \alpha$ & $ \alpha\left(\eta N_{\zeta_k} + 1 \right)$ & $\alpha \left( \frac{\eta N_{\zeta_k} + 1}{1 - \beta_k} \right)$ & $h_{j,k}\left(N_{\zeta_k},r\right) \bm{I} + \gamma \bm{A} \odot \bm{B}$ \\\hline
	Initial condition $g_{j,k}\left(\cdot \right)$ &$1$ & $e^{-\delta N_{\zeta_k}}$ & $(1-\beta_k) e^{-\delta N_{\zeta_k}}$ & $(1-\beta_k) e^{-\delta r N_{\zeta_k}} \bm{I}$ \\\hline
\end{tabular}
\end{adjustbox}
\end{center}	
\end{table}
% ---
% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Results}\label{sec_use_case}
Let a skill learning scenario be defined via the tuple $\phi = \left(N_\mathcal{S}, N_\mathcal{K}, m, \bm{\rho} \right) \in \Phi$, where $N_\mathcal{S} > N_\mathcal{K}$ and $m >> 1$, and $\Phi$ represents the set of all possible combinations. The parameter vector $\bm{\rho} = \left[\alpha, \eta, \delta\right]$ defines the knowledge exchange efficiency of the particular scenario. Notice that the generality of $\Phi$ can very well be representative of a smart factory scenario where multiple robots learn different manufacturing tasks, a home crew of service robots learn different chores, or a fleet of underwater robots performing exploration, inspection, and maintenance routines. The different hypothetical scenarios posed by $\Phi$ allow contrasting the different learning paradigms regarding their associated energy demand (related to the CCE, BEE, and MIE expenditure categories).
	
Concretely, to resemble the conditions implied in Assumptions~\ref{assumption:average_behavior}, \ref{assumption:agent_similarity},~\ref{assumption:cluster_size}, and~\ref{assumption:cluster_transferability}, a prototypical $\phi_{SF}$ involves several robots performing multiple skills across different clusters. In our particular instance of $\phi_{SF}$ we have $m=32$ robots tasked with mastering a pool $\mathcal{S}$ of $N_\mathcal{S}= 512$ skills segregated into $N_\mathcal{K}=4$ clusters of $N_\mathcal{Z} = 128$ each. A given skill is considered learned when the remaining knowledge $\bar{\sigma}$ goes below the threshold $\epsilon = 0.01$. The fundamental skill complexity for all the skills in $\mathcal{S}$ is $c_0 = 100$ episodes. The elements of the vector $\bm{\rho}$ are chosen to be $\alpha =  0.0461$ ---in accordance to Eq.~\eqref{eq:isolated_learning_rate}---, $\delta =  0.0360$ ---see Eq.~\eqref{eq:delta}---, and $\eta= 0.1$. Via the conditions posed by $ \phi_{SF}$, we will show the advantages of using CL on reducing the complexity of skill learning and thereby reducing the energy consumed by learning the skills in $\mathcal{S} $.

The power-per-episode (see Asm.~\ref{assumption:power_and_episode_time}) is determined by the sum of the power required for basal processes, the power for motion and interaction, and the power for computation and communication, i.e.
% ---
\begin{equation}
	P_0 = P_{BEE}+P_{MIE} + P_{CCE}.
\end{equation}
% ---
To assign a numerical value to $P_{BEE}$, and without loss of generality, we consider $\phi_{SF}$ an instance of a smart factory populated with state-of-the-art tactile robots, like those listed in Sec.~\ref{sec:app_cobot_ener_consumption}, which require a typical power of about $\unit[40]{W}$. To approximate $P_{MIE}$, we use the fact that, in demanding tasks, the power demand of a cobot can be upper-bounded at around $ \unit[300] {W} $. Finally, to determine $P_{CCE}$, we assume that, to deal with the computing effort that learning new skills will have on the robots' local processors, the smart factory will delegate the computational burden to a remote computing unit, i.e., cloud computing. Thus, we take as reference the work in \cite{Strubell2019EnergyPolicyConsiderations}, where a state-of-the-art machine learning algorithm executed in a cluster required $\unit[1,415.78]{W}$ to solve a task. Finally, we can assume that executing each trial episode $n$ takes $\Delta t = 60$ seconds. Using these reference values, we can estimate that, when learning a skill, an average trial episode has an energetic demand of:
% ---
%\begin{equation}
%	e_0 = P_0 \Delta t = \left(40 + 300 + 1,415.78\right) \left(60\right) \approx 105~\text{kJ}.
%\end{equation}
\begin{equation}
	e_0 = P_0 \Delta t \approx 105~\text{kJ}.
\end{equation}
% ---
% ===================================================================================================
\paragraph*{The skill complexity of the different paradigms}
The remaining knowledge for the four skills learned per robot is shown in Fig.~\ref{fig:collective_learning} in logarithmic scale. The $m$ robots are used to learn in parallel the $N_\mathcal{Z}$ skills of each cluster in succession, as shown in Fig.~\ref{fig:cluster_learning_sequence}. Notice that, as expected, IsL (Fig.~\ref{fig:dynamics_isolated_learning}) exhibits the worst performance, always requiring $c_0$ episodes to learn every skill. Since IL (Fig.~\ref{fig:dynamics_incremental_learning}) does not benefit from the knowledge from the previously visited clusters, a robot $r_i$ needs to start accumulating knowledge from the beginning every time it moves to a different cluster. This is not the case in TIL (Fig.~\ref{fig:dynamics_incremental_transfer_learning}), as the more clusters a robot has visited, the faster a new skill is learned. The speed of knowledge collection is exponentiated with CL (Fig.~\ref{fig:dynamics_collective_learning}) thanks to the exchange of knowledge among the $m$ robots. Compared to the other learning paradigms, with CL, the skills are learned within a few trial episodes in every cluster. 

To assess how the number $m$ of robots affects the total number of trial episodes $C_\mathcal{S}$ required to learn all the $N_\mathcal{S}$ skills, we use the same parameters as before but vary $m \in \left \lbrace 2,4,8,16,32,64,128\right \rbrace$. Moreover, we considered an additional CL scenario in which, unlike the previous case, the total number of available robots is distributed equally among the clusters to benefit from transfer learning at an earlier time during learning. The results are shown in Fig.~\ref{fig:total_episodes_per_n_robots}. It can be seen that, at first, IL is better than the trivial IsL case; however, as the number of robots increases, the skill knowledge is divided among the available robots, which implies that less knowledge can be passed as the pool of learned sills $\zeta_k$ per robot gets smaller. This explains why the total number of trial episodes for IsL and IL approach each other in the limit. With a growing robot number, TIL exhibits a similar behavior. Less cluster knowledge can be collected by each robot and transferred to the next cluster. Indeed, TIL rapidly converges to IL and eventually to IsL. In CL, a similar effect shows that when all robots learn skills from the same clusters, as the number of robots grows, the total complexity approaches that of the same number of robots distributed across clusters.
% ---
\begin{figure*}[!h]
	\centering
	\hspace*{\fill}
	\begin{subfigure}[t]{0.45\textwidth}
		\subcaption{}
		\includegraphics[width= \textwidth]{total_episodes_per_n_robots.png} \label{fig:total_episodes_per_n_robots}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.45\textwidth}
		\subcaption{}
		\includegraphics[width=\textwidth]{total_energy_per_n_robots.png} \label{fig:total_energy_per_n_robots}
	\end{subfigure}
	\hspace*{\fill}
	\caption[] {\label{fig:final_results} The effect of the number of robots: \subref{fig:total_episodes_per_n_robots} Total number of episodes to learn the universe of skills as a function of the available robots and \subref{fig:total_energy_per_n_robots} the total energy consumption.}
\end{figure*}
% ---

% SUBSECTION ========================================================================================
\paragraph*{Energy consumption}
Consider that the results in Fig.~\ref{fig:total_episodes_per_n_robots} show the total number of episodes required by each of the $m$ robots. To compute the total energy demand, those numbers need to be scaled by the factor $m e_0$, which leads to the consumption shown in Fig.~\ref{fig:total_energy_per_n_robots}. Undoubtedly, CL shows that it has not only the best energy usage of all the paradigms but, unlike the rest, the more robots take part in learning the universe of skills, the better overall energy usage.

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Discussion}\label{sec:discussion}
While the unprecedented strides in AI and robotics have revolutionized numerous sectors, the ongoing proliferation and associated energy consumption cannot be ignored. As the scope of AI continues to expand, a concerted effort is required to strike a balance between innovation and conscientious energy usage to steer AI toward sustainable operation. To emphasize the energy consumption's significance in AI systems, we introduced three principal energy expenditure categories. We juxtaposed them with the grand challenges stemming from the escalating CAI applications and growing population of EAI agents. In particular, we underscored that mitigating energy consumption in EAI systems requires enhanced mechanical designs, efficient computation and communication hardware, and a paramount emphasis on harnessing the simultaneous sharing, exchange, transfer, and accumulation of knowledge acquired by the various agents.

% ===================================================================================================
\paragraph*{Developing collective learning to address the energy challenges of EAI}
Fig.~\ref{fig:challengesConnected} depicts the natural connections between the energy grand challenges associated with EAI. This allows us to identify important areas of opportunity for collective learning. Directly related to challenge C1 and the computation and communication energy expenditure ($E_{CCE}$), the EAI research community stands to gain significant headway by channeling efforts into realizing collective learning algorithms. This might involve focusing on data-efficient methodologies, infusing pertinent prior knowledge into models, or fostering knowledge-sharing capabilities. The latter emerges as an especially remarkable solution (as shown in our simulation study), poised to expedite multi-skill learning by leveraging a cloud-connected repository of skills. With time, this paradigm shift could transform data center demands from compute-intensive to storage and querying, drastically reducing energy needs, apart from the compelling need to advance computational algorithms to leverage efficient hardware and streamline communication protocols. Challenge C2 is intricately interwoven with the surging population of active robots and other intelligent machines. The relevance of better mechatronic designs (e.g., lightweight materials, flexible components, and energy-efficient actuation) for fine-tuning energy utilization during skill execution is obvious and a problem by itself. Collective learning can contribute to reducing the basal energy expenditure ($E_{BEE}$) and the energy implicated in motion and interaction ($E_{MIE}$) by reducing the time dedicated to learning and executing skills thanks to the body of knowledge collected by the multitude of agents with similar capabilities. Finally, the efforts dedicated to approaching challenges C1 and C2 will ripple into advancements in C3. As mentioned before, recycling is a supplementary avenue for energy optimization in C3. Integrating recycling within the manufacturing landscape of machines would enable the reclamation of usable parts from retired robots and the reutilization of materials from discarded components. As the journey towards energy-efficient EAI unfolds, a holistic approach marrying innovative algorithms, efficient hardware, sustainable designs, and recycling endeavors offers a beacon of promise.
% ---
\begin{figure}[!t]
	\centering
	\includegraphics[width=0.45\textwidth]{fig/grand_challenges_connections.png}
	\caption{Interconnection between challenges C1, C2, and C3.}
	\label{fig:challengesConnected}
\end{figure}
% ---

% ===================================================================================================
\paragraph*{Closing remarks}
As discussed in \cite{Kaelbling2020foundationefficientrobot}, efficient robotic learning algorithms enabling agents to acquire new skills on the fly must possess specific key attributes: sample efficiency, generalizability, compositionality, and incremental learning capabilities. The CL paradigm inherently fulfills these prerequisites by harnessing the full communication potential of networked EAI agents. This approach facilitates real-time concurrent knowledge exchange and aggregation, resulting in energy- and time-efficient skill acquisition.

Our results suggest that utilizing the conventional paradigms of isolated, incremental, and transfer learning on many EAI agents does not lead to optimal energy utilization. This remains true even when multiple agents operate concurrently since, in the absence of true inter-agent knowledge exchange, energy requirements increase substantially as the number of EAI agents grows. Conversely, our simulation study highlighted that collective learning presents a solution to energy demands contingent on effectively sharing knowledge based on skill similarity. Notably, the CL paradigm showcased superior performance with increasing robot numbers, enabling concurrent acquisition of multiple skills.

While collective learning holds significant promise, it is crucial to recognize that the fundamental algorithms and infrastructure necessary to make it a reality are either nonexistent or in active development. Furthermore, contemporary state-of-the-art algorithms focusing on proper incremental and transfer learning are still in the early stages of development. However, although our primary focus has centered on how collective learning addresses the formidable energy challenges posed by EAI, it is essential to acknowledge that its potential extends far beyond this specific domain.

The collective learning paradigm is equally applicable to CAI agents. Indeed, recent developments have shed light on the potential of edge computing and federated learning, wherein computational tasks are distributed beyond the confines of centralized data centers to multiple CAI agents. Furthermore, foundational models developed through extensive research and learning now serve as the foundational blocks for solving more specific, nuanced tasks, remarking the power of true knowledge transfer.

Much like in the context of EAI, the promise of collective learning for CAI becomes evident if efficient means to exchange and aggregate knowledge from CAI agents, each running its learning routines, are established. The synergies facilitated by the CL approach have the potential to significantly enhance the problem-solving capabilities and energy efficiency of CAI applications. This ultimately underscores the versatility and potential impact of collective learning across the spectrum of artificial intelligence domains. We hope the arguments in this discussion catalyze further research endeavors, ultimately bringing collective learning to fruition.

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section*{Supplementary Materials}
Sections \ref{sec:materials_and_methods} to \ref{sec:app_robot_ener_consumption}\\
Fig.~\ref{fig:power_per_episode} to Fig.~\ref{fig:cobot_watt_per_kg}

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\renewcommand\refname{References and Notes}
\bibliography{bib/References.bib}
\bibliographystyle{Science}

%\begin{thebibliography}{10}
%	
%	\bibitem{Szczepanski2019Economicimpactsartificial}
%	M.~Szczepanski, Economic impacts of artificial intelligence ({AI}) (2019).
%	
%	\bibitem{Strubell2019EnergyPolicyConsiderations}
%	E.~Strubell, A.~Ganesh, A.~McCallum, {\it Energy and Policy Considerations for
%		Deep Learning in NLP\/}, {\it ACL\/} (2019).
%	
%	\bibitem{Cao2020TowardsAccurateReliable}
%	Q.~Cao, A.~Balasubramanian, N.~Balasubramanian, {\it Towards Accurate and
%		Reliable Energy Measurement of {NLP} Models\/}, {\it Proceedings of
%		SustaiNLP: Workshop on Simple and Efficient Natural Language Processing\/}
%	(Association for Computational Linguistics, Online, 2020), pp. 141--148.
%	
%	\bibitem{Chebotar2019Closingsimreal}
%	Y.~Chebotar, {\it et~al.\/}, {\it Closing the sim-to-real loop: Adapting
%		simulation randomization with real world experience\/}, {\it 2019
%		International Conference on Robotics and Automation (ICRA)\/} (IEEE, 2019),
%	pp. 8973--8979.
%	
%	\bibitem{Lehdonvirta2022futuresunpaidwork}
%	V.~Lehdonvirta, L.~P. Shi, E.~Hertog, N.~Nagase, Y.~Ohta, {\it The future (s)
%		of unpaid work: How susceptible do experts from different backgrounds think
%		the domestic sphere is to automation?\/}, {\it Plos one\/} {\bf 18}, e0281282
%	(2023).
%	
%	\bibitem{andrae2015global}
%	A.~S. Andrae, T.~Edler, {\it On global electricity usage of communication
%		technology: trends to 2030\/}, {\it Challenges\/} {\bf 6}, 117 (2015).
%	
%	\bibitem{Hintemann2022Cloudcomputingdrives}
%	R.~Hintemann, S.~Hinterholzer, Cloud computing drives the growth of the data
%	center industry and its energy consumption (2022).
%	
%	\bibitem{schwartz2019green}
%	R.~Schwartz, J.~Dodge, N.~A. Smith, O.~Etzioni, Green ai (2019).
%	
%	\bibitem{vinuesa2020role}
%	R.~Vinuesa, {\it et~al.\/}, {\it The role of artificial intelligence in
%		achieving the {S}ustainable {D}evelopment {G}oals\/}, {\it Nature
%		Communications\/} {\bf 11}, 1 (2020).
%	
%	\bibitem{zhou2020hulk}
%	X.~Zhou, Z.~Chen, X.~Jin, W.~Y. Wang, {\it HULK: An Energy Efficiency Benchmark
%		Platform for Responsible Natural Language Processing\/}, {\it arXiv preprint
%		arXiv:2002.05829\/}  (2020).
%	
%	\bibitem{Dalgren2019GreenMLA}
%	A.~Dalgren, Y.~Lundeg{\aa}rd, {\it GreenML : A methodology for fair evaluation
%		of machine learning algorithms with respect to resource consumption\/}
%	(2019).
%	
%	\bibitem{garcia2019estimation}
%	E.~Garc{\'\i}a-Mart{\'\i}n, C.~F. Rodrigues, G.~Riley, H.~Grahn, {\it
%		Estimation of energy consumption in machine learning\/}, {\it Journal of
%		Parallel and Distributed Computing\/} {\bf 134}, 75 (2019).
%	
%	\bibitem{real2019regularized}
%	E.~Real, A.~Aggarwal, Y.~Huang, Q.~V. Le, {\it Regularized evolution for image
%		classifier architecture search\/}, {\it Proceedings of the aaai conference on
%		artificial intelligence\/} (2019), pp. 4780--4789.
%	
%	\bibitem{krizhevsky2012imagenet}
%	A.~Krizhevsky, I.~Sutskever, G.~E. Hinton, {\it Imagenet classification with
%		deep convolutional neural networks\/}, {\it Advances in neural information
%		processing systems\/} {\bf 25}, 1097 (2012).
%	
%	\bibitem{IFR2019}
%	{\relax International Federation of Robotics}, {\it World Robotics 2019
%		Industrial Robots\/} (IFR Statistical Department, 2019).
%	
%	\bibitem{sirkin2015}
%	H.~L. Sirkin, M.~Zinser, J.~Rose, How robots will redefine competitiveness
%	(2015). Retrieved March 8, 2016 from: \url{https://goo.gl/YxPfyF}.
%	
%	\bibitem{fraunhofer2016}
%	{\relax Fraunhofer ISE}, Net installed electricity generation capacity in
%	germany. Retrieved March 9, 2016 from:
%	\url{https://www.energy-charts.de/power_inst.htm}.
%	
%	\bibitem{tobe2015}
%	F.~Tobe, Why co-bots will be a huge innovation and growth driver for robotics
%	industry (2015). Retrieved April 5, 2016 from: \url{http://goo.gl/hRG5Du}.
%	
%	\bibitem{IFR2015}
%	{\relax International Federation of Robotics}, Service robot statistics.
%	Retrieved April 5, 2016 from:
%	\url{http://www.ifr.org/service-robots/statistics/}.
%	
%	\bibitem{schroder2014}
%	S.~Schr\"oder, Optimized movements: Ballet of the bots (2014). Retrieved March
%	8, 2016 from: \url{http://goo.gl/0Ir231}.
%	
%	\bibitem{chalmers2015}
%	{\relax Chalmers University of Technology}, Smooth robot movements reduce
%	energy consumption by up to 40 percent (2015). Retrieved March 8, 2016 from:
%	\url{www.sciencedaily.com/releases/2015/08/150824064923.htm}.
%	
%	\bibitem{mohammed2014}
%	A.~Mohammed, B.~Schmidt, L.~Wang, L.~Gao, {\it Minimizing Energy Consumption
%		for Robot Arm Movement\/}, {\it Procedia CIRP\/} {\bf 25}, 400 (2014).
%	
%	\bibitem{chemnitz2011}
%	M.~Chemnitz, G.~Schreck, J.~Krger, {\it Analyzing energy consumption of
%		industrial robots\/}, {\it Emerging Technologies Factory Automation (ETFA),
%		2011 IEEE 16th Conference on\/} (2011), pp. 1--4.
%	
%	\bibitem{Haddadin2014SystemzumErstellen}
%	S.~Haddadin, System zum erstellen von steuerungsdatens\"atzen f\"ur roboter
%	(2014). German Patent {DE} 10 2014 112 639 B4 2018.02.08.
%	
%	\bibitem{Haddadin2015Systemgeneratingsets}
%	S.~Haddadin, System for generating sets of control data for robots (2015).
%	European Patent {EP} 3 189 385 {B}1.
%	
%	\bibitem{Garavan2012CollectiveLearning}
%	T.~N. Garavan, R.~Carbery, {\it Collective Learning\/} (Springer US, Boston,
%	MA, 2012), pp. 646--649.
%	
%	\bibitem{levine2018learning}
%	S.~Levine, P.~Pastor, A.~Krizhevsky, J.~Ibarz, D.~Quillen, {\it Learning
%		hand-eye coordination for robotic grasping with deep learning and large-scale
%		data collection\/}, {\it The International journal of robotics research\/}
%	{\bf 37}, 421 (2018).
%	
%	\bibitem{rudin2022learning}
%	N.~Rudin, D.~Hoeller, P.~Reist, M.~Hutter, {\it Learning to walk in minutes
%		using massively parallel deep reinforcement learning\/}, {\it Conference on
%		Robot Learning\/} (PMLR, 2022), pp. 91--100.
%	
%	\bibitem{flairop2023}
%	K.~I. f\"ur Technologie, {FLAIROP: Federated Learning for Robotic Picking},
%	\url{https://flairop.com/} (2023).
%	
%	\bibitem{Kaelbling2020foundationefficientrobot}
%	L.~P. Kaelbling, {\it The foundation of efficient robot learning\/}, {\it
%		Science\/} {\bf 369}, 915 (2020).
%	
%	\bibitem{statista_ir_cobot_share}
%	Statista, Share of traditional and collaborative robot unit sales worldwide
%	from 2018 to 2022 (2020).
%	
%	\bibitem{montaqim2015}
%	A.~Montaqim, Top 9 industrial robot companies and how many robots they have
%	around the world (2015). Retrieved March 8, 2016 from:
%	\url{http://goo.gl/QEIBr2}.
%	
%	\bibitem{fanuc2015}
%	{\relax FANUC America}, Fanuc announces record-breaking 400,000 robots sold
%	worldwide (2015). Retrieved March 8, 2016 from:
%	\url{http://www.fanucamerica.com/FanucAmerica-news/Press-releases/PressReleaseDetails.aspx?id=76}.
%	
%	\bibitem{yaskawa2014}
%	{\relax Motoman}, 7 things you may not know about yaskawa (2014). Retrieved
%	March 8, 2016 from:
%	\url{http://www.motoman.com/blog/index.php/7-things-may-know-yaskawa/}.
%	
%	\bibitem{ABB2015}
%	{\relax ABB}, {ABB Robotics} (2015). Retrieved March 8, 2016 from:
%	\url{http://new.abb.com/products/robotics}.
%	
%	\bibitem{statista_ir_operational_stock}
%	Statista, Operational stock of multipurpose industrial robots worldwide from
%	2010 to 2020 (2023).
%	
%	\bibitem{Heredia2023BreakingEnergyConsumption}
%	J.~Heredia, C.~Schlette, M.~B. Kj{\ae}rgaard, {\it Breaking Down the Energy
%		Consumption of Industrial and Collaborative Robots: A Comparative Study\/},
%	{\it IEEE International Conference on Emerging Technologies and Factory
%		Automation\/} (IEEE, 2023).
%	
%\end{thebibliography}
% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\textbf{Acknowledgments:}
We thank Carlos Magno C. O. Valle for his feedback and support throughout the research process. \textbf{Funding:} The authors greatly acknowledge the funding of this work by the Alfried Krupp von Bohlen und Halbach Foundation. \textbf{Author contributions:} S. Haddadin developed the fundamental collective learning concept and hypothesized its learning acceleration and minimizing energy consumption effects.  The mathematical framework was developed by S. Haddadin and F. Daz Ledezma. F. Daz Ledezma implemented and conducted all of the experiments and analyzed the data. F. Daz Ledezma and S. Haddadin interpreted the results. S. Haddadin and F. Daz Ledezma conceptualized, F. Daz Ledezma wrote, and S. Haddadin revised and edited the manuscript. All of the authors read the paper. \textbf{Competing interests:} The authors declare that there are no potential conflicts of interest. \textbf{Data and materials availability:} All data needed to evaluate the conclusions in the paper are present in the main manuscript or the Supplementary Materials. %The datasets generated and analyzed in the current study are available at \url{https://github.com/mecafdl/pigraphs_body_morphology}. Requests for additional materials should be addressed to S. Haddadin.

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
 \newpage
 \beginsupplement
 \section*{Supplementary Materials}\label{sec:supplementary_materials}
 \input{supplementary.tex}

\end{document}