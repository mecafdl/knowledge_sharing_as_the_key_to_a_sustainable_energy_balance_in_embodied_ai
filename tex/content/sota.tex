%  SUBSECTION ========================================================================================
% \hrulefill
% \subsection{The energy demand of machine learning}
% %\hl{In this relevant?}\textcolor{red}{The past decade witnessed unprecedented advances in machine learning research. When a first iteration of a model to solve a given problem is proposed, subsequent approaches follow that try to improve on the performance of its predecessor based on a defined metric. Such metric usually regards the accuracy of the solution to test data. Since there are no extra constraints on how models should be developed, there is a natural exploitation of certain variables of the problem to improve the given performance. {The main problem of this is when some of the explored variables blow out of reasonable proportions and start to impact on other scientific fields}.}
% Recently, there has been an increasing concern about the negative impact that artificial intelligence, and machine learning, can have on the environment. Such concerns have been expressed in news articles \cite{kelly_2019,hao_2019, kaminska_2014, knight_2020, talwalkar_2020, ekin_2019} as well as in scientific publications like \cite{Strubell2019EnergyAP}. The latter highly influential work discussed the financial and environmental toll of deep learning, particularly from off-the-shelf natural language processing (NLP) methods. The study describes how modern NLP techniques have rendered advances at the expense of data-intensive algorithms that run on costly and energy-demanding data centers. The authors approximated the total sum of resources required by state-of-the-art algorithms regarding tuning and experimentation stages and expressed it as carbon dioxide emissions. Similarly, in \cite{zhou2020hulk} authors present a benchmark for the study of energy efficiency in NLP algorithms. Several classical benchmark datasets are used. The authors quantify the energy efficiency of the algorithms' development phases, including model pretraining, fine-tuning, and inference in terms of the pretraining time, pretraining cost, training time, training cost, inference time, inference latency and cost required to achieve a certain performance. 

% An appeal to observe the efficiency of deep learning algorithms as a measure to develop environmentally-aware AI consumption was made in \cite{schwartz2019green}. The authors point to the fact that the number of computations required by deep learning algorithms has constantly increased in recent times, more specifically, 300000x in the last 6 years. This has as a consequence an increased carbon footprint caused by the data centers used for these computations. Authors introduce \emph{redAI} as algorithms that produce an increase in the accuracy of the results at the expense of huge computational power. Similarly, they defined \emph{greenAI} as research that produces an improvement in accuracy without increasing computational cost. The authors propose the total number of floating-point operations (FPO) as a metric for computational efficiency. The work in \cite{vinuesa2020role} discussed documented connections between AI acting as an enabler or inhibitor for Sustainable Development Goals (SDGs). Here the large energy demands of the data centers used to support modern AI algorithms are regarded as an indicator of negative effects that AI can have on the environment. Authors mention that energy-efficient AI requires not only energy-efficient data centers running on renewable energy but also embedding human knowledge in the development of AI models.

% As awareness on the impact of AI on the environment raises, efforts have been made to define metrics that capture this effect. For instance, metrics to assess resource consumption of machine learning algorithms are discussed in \cite{Dalgren2019GreenMLA}. Here five efficiency metrics are introduced that consider accuracy, model size, time and CPU/GPU energy consumption for the training and inference phases. In \cite{garcia2019estimation} the authors provided a review of different methods to estimate energy consumption aimed specifically at the machine learning research community. They present different approaches at a system level, such as performance counters (PMC), simulation, real-time power estimation, instruction-level estimation, and a hardware-level estimation and evaluated a set of state of the art machine learning algorithms with them. 

% In summary, the key idea learned from the aforementioned publications is that efforts toward reducing the algorithms' energy requirements should be incentivized, i.e. model complexity, parameter search requirements, scalability, and transferability should be balanced in efficiency-driven metrics. However, despite these recent efforts in the machine learning community, embodied AI researchers had not yet paid enough attention to this problem

%\subsection{Energy demand in robotics}\label{sec:energy_in_robotics}
%Energy consumption has typically been an aspect under consideration in industrial processes. Robotics and automation, in turn, have been seen as means to increase the efficiency of those processes. The quintessential example of this is an industrial robot performing precisely and efficiently one (or many) manufacturing tasks. Nevertheless, as for every other machine, energy efficiency is an important factor to consider in industrial robots. As technology progresses, robots have increased the energetic efficiency of their hardware from the design stage. This efficiency is reflected in their power to payload ratio, which is a measure of how much energy a robot consumes by moving a nominal load along a standard trajectory. This improvement is shown in Fig.~\ref{fig:powerPayloadRatioFlagshipRobots} which depicts the power/payload ratio for flagship robots across the generations of robots. Evidently, there has been a marked improvement in this area. Further gains in the energy efficiency of robots have been achieved via better control systems and machine learning algorithms that allow robots to execute tasks in time and energy optimal ways. However, when looking at the energy demand required from robots, analysis tends to focus on the individual systems and the fact that the number of robots are rising rapidly usually escapes attention, see Fig.~\ref{fig:robotics_energy_trend}. Looking back at the power-to-payload ratio, this individual betterment pales in comparison to the numbers of robots that are expected to be installed in the coming years. Therefore, one simple, yet mostly ignored, conclusion can be drawn: \emph{more robots, more energy demand}.

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section{State of the art}
As mentioned previously, all the benefits that embodied AI will bring about as it permeates the smart factory will have their toll, in particular, in the form of energy consumption. Here, we briefly go over some of the few recent works what have looked at this issue and that focus on either of the two most relevant components of embodied AI: machine learning and robotics. The former relates to the CCE category while the latter pertains to the BBE and MIE types.

% SUBSECTION ========================================================================================
\subsection{Energy demand in machine learning}
Recently, there has been an increasing concern about the negative impact that artificial intelligence and machine learning can have on the environment. For instance, \cite{schwartz2019green} discusses the efficiency of deep learning algorithms as a measure to develop environmentally-aware AI. The study states that the number of computations required by deep learning algorithms has increased 300000x in the last 6 years; having as a consequence an increased carbon footprint caused by the \textbf{data centers} used for these computations. Likewise, the work in \cite{vinuesa2020role} presents a similar study linking the negative effects of AI on the environment to the the large energy demands of the required \textbf{data centers}. In the influential work \cite{Strubell2019EnergyAP}, authors addressed the environmental toll from state-of-the-art data-intensive natural language processing (NLP) algorithms and expressed it as carbon dioxide emissions resulting from running the algorithms on costly and energy-demanding \textbf{data centers}. Similarly, in \cite{zhou2020hulk} authors present a benchmark for the study of energy efficiency in NLP algorithms; quantifying it in terms of the cost corresponding to the development phases. Other related works define metrics to capture this effect of AI on the environment. For instance, five efficiency metrics are introduced in \cite{Dalgren2019GreenMLA} to assess resource consumption of machine learning algorithms that consider accuracy, model size, time and CPU/GPU energy consumption for the training and inference phases. \cite{garcia2019estimation} presents different approaches at a system level to estimate the energy consumption of a set of state-of-the-art machine learning algorithms. The considered approaches include performance counters (PMC), real-time power estimation, instruction-level estimation, and a hardware-level estimation among others. \hl{It is clear that, despite the apparent awareness of part of the research community regarding energy consumption, there is a lack of general effort in the full research community to address those issues.}   


% SUBSECTION ========================================================================================
\subsection{Energy demand in robotics}\label{sec:energy_in_robotics}
Recent statistics \cite{IFR2019} show a clear increasing trend in the worldwide demand for industrial robots. Naturally, the rising numbers of operational units are directly linked to a significant increase in the consumption of electric energy. Reduction of the energy consumption from industrial robots has been studied in recent works, see \cite{schroder2014, chalmers2015, mohammed2014, chemnitz2011}. Similarly, research has focused on exploiting robots with elastic actuation to make more efficient use of energy \cite{scalera2019natural, carabin2017review, bolivar2017general, haddadin2011optimal,haddadin2012intrinsically}. Moreover, research initiatives have worked on methods to make intelligent manufacturing more efficient, e.g., \cite{aerus2014, bukata2016energy}. For instance, a review of various methods and technologies for improving energetic performance of industrial robots was presented in \cite{carabin2017review}. It covers aspects such as the appropriate selection of robot type as well as additions and replacements of hardware, such as for the storage and sharing of energy. Software methods are also discussed, center at enhanced motion planning, via trajectory optimization and operation scheduling. Albeit these initial approaches, there is a lack of studies that consider the rising number of operational industrial robots and the direct effect that they will have in the electric energy consumption worldwide. There is, however, a relevant related study focused on the robotics market \cite{barnett_2017} in the United States. It projected the future demand for electricity that the ever-increasing introduction of robot in many aspects of human life will originate based on the energy interactions for robot types across various sectors. The study estimates a demand of 22,822 GWh in 2025. Authors highlight, among other aspects, the need of increased R\&D to develop optimized motion patterns to reduce energy consumption while maximizing performance. In spite of the contribution of these works. there is no clear estimate of the energy demand rise that the increasing number of robot installations will bring into the picture nor a strategy to address this problem.

In summary, the aforementioned publications show that efforts in separate fronts have tackled the energy consumption of different aspects of embodied. However, no previous study has given a holistic approach to the energy consumption problem in embodied AI. We attempt to bridge this gap in this work by directly addressing the energy challenges posed by the advent of embodied AI and ponder collective learning as the core element that can leverage scalability, and transferability; thus achieving energy efficiency leveraging the components of embodied AI.