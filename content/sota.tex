%  SUBSECTION ========================================================================================
% \hrulefill
% \subsection{The energy demand of machine learning}
% %\hl{In this relevant?}\textcolor{red}{The past decade witnessed unprecedented advances in machine learning research. When a first iteration of a model to solve a given problem is proposed, subsequent approaches follow that try to improve on the performance of its predecessor based on a defined metric. Such metric usually regards the accuracy of the solution to test data. Since there are no extra constraints on how models should be developed, there is a natural exploitation of certain variables of the problem to improve the given performance. {The main problem of this is when some of the explored variables blow out of reasonable proportions and start to impact on other scientific fields}.}
% Recently, there has been an increasing concern about the negative impact that artificial intelligence, and machine learning, can have on the environment. Such concerns have been expressed in news articles \cite{kelly_2019,hao_2019, kaminska_2014, knight_2020, talwalkar_2020, ekin_2019} as well as in scientific publications like \cite{Strubell2019EnergyAP}. The latter highly influential work discussed the financial and environmental toll of deep learning, particularly from off-the-shelf natural language processing (NLP) methods. The study describes how modern NLP techniques have rendered advances at the expense of data-intensive algorithms that run on costly and energy-demanding data centers. The authors approximated the total sum of resources required by state-of-the-art algorithms regarding tuning and experimentation stages and expressed it as carbon dioxide emissions. Similarly, in \cite{zhou2020hulk} authors present a benchmark for the study of energy efficiency in NLP algorithms. Several classical benchmark datasets are used. The authors quantify the energy efficiency of the algorithms' development phases, including model pretraining, fine-tuning, and inference in terms of the pretraining time, pretraining cost, training time, training cost, inference time, inference latency and cost required to achieve a certain performance. 

% An appeal to observe the efficiency of deep learning algorithms as a measure to develop environmentally-aware AI consumption was made in \cite{schwartz2019green}. The authors point to the fact that the number of computations required by deep learning algorithms has constantly increased in recent times, more specifically, 300000x in the last 6 years. This has as a consequence an increased carbon footprint caused by the data centers used for these computations. Authors introduce \emph{redAI} as algorithms that produce an increase in the accuracy of the results at the expense of huge computational power. Similarly, they defined \emph{greenAI} as research that produces an improvement in accuracy without increasing computational cost. The authors propose the total number of floating-point operations (FPO) as a metric for computational efficiency. The work in \cite{vinuesa2020role} discussed documented connections between AI acting as an enabler or inhibitor for Sustainable Development Goals (SDGs). Here the large energy demands of the data centers used to support modern AI algorithms are regarded as an indicator of negative effects that AI can have on the environment. Authors mention that energy-efficient AI requires not only energy-efficient data centers running on renewable energy but also embedding human knowledge in the development of AI models.

% As awareness on the impact of AI on the environment raises, efforts have been made to define metrics that capture this effect. For instance, metrics to assess resource consumption of machine learning algorithms are discussed in \cite{Dalgren2019GreenMLA}. Here five efficiency metrics are introduced that consider accuracy, model size, time and CPU/GPU energy consumption for the training and inference phases. In \cite{garcia2019estimation} the authors provided a review of different methods to estimate energy consumption aimed specifically at the machine learning research community. They present different approaches at a system level, such as performance counters (PMC), simulation, real-time power estimation, instruction-level estimation, and a hardware-level estimation and evaluated a set of state of the art machine learning algorithms with them. 

% In summary, the key idea learned from the aforementioned publications is that efforts toward reducing the algorithms' energy requirements should be incentivized, i.e. model complexity, parameter search requirements, scalability, and transferability should be balanced in efficiency-driven metrics. However, despite these recent efforts in the machine learning community, embodied AI researchers had not yet paid enough attention to this problem

%\subsection{Energy demand in robotics}\label{sec:energy_in_robotics}
%Energy consumption has typically been an aspect under consideration in industrial processes. Robotics and automation, in turn, have been seen as means to increase the efficiency of those processes. The quintessential example of this is an industrial robot performing precisely and efficiently one (or many) manufacturing tasks. Nevertheless, as for every other machine, energy efficiency is an important factor to consider in industrial robots. As technology progresses, robots have increased the energetic efficiency of their hardware from the design stage. This efficiency is reflected in their power to payload ratio, which is a measure of how much energy a robot consumes by moving a nominal load along a standard trajectory. This improvement is shown in Fig.~\ref{fig:powerPayloadRatioFlagshipRobots} which depicts the power/payload ratio for flagship robots across the generations of robots. Evidently, there has been a marked improvement in this area. Further gains in the energy efficiency of robots have been achieved via better control systems and machine learning algorithms that allow robots to execute tasks in time and energy optimal ways. However, when looking at the energy demand required from robots, analysis tends to focus on the individual systems and the fact that the number of robots are rising rapidly usually escapes attention, see Fig.~\ref{fig:robotics_energy_trend}. Looking back at the power-to-payload ratio, this individual betterment pales in comparison to the numbers of robots that are expected to be installed in the coming years. Therefore, one simple, yet mostly ignored, conclusion can be drawn: \emph{more robots, more energy demand}.

% ===================================================================================================
%                                                 |                                                 |
%                                                 |                                                 |
% -------------------------------------------- SECTION ---------------------------------------------|
%                                                 |                                                 |
%                                                 |                                                 |
% ===================================================================================================
\section{Energy demand of Embodied AI}
The benefits that embodied AI will bring about as it permeates the smart factory will take a toll, in particular, on the energy demand. We briefly go over some of the few recent works what have looked at this issue and that focus on either of the two most relevant components of embodied AI: machine learning and robotics, with the former directly connected to the CCE energy usage category and the latter corresponding to the BBE and MIE categories.

% SUBSECTION ========================================================================================
\subsection{Energy demand in machine learning}
%Recently, there has been an increasing concern in the research community about the negative impact that artificial intelligence and machine learning can have on the environment. For instance, \cite{schwartz2019green} discusses the efficiency of deep learning algorithms as a measure to develop environmentally-aware AI. The study states that the number of computations required by deep learning algorithms has increased 300000x in the last 6 years; having as a consequence an increased carbon footprint caused by the data centers used for these computations. A similar study \cite{vinuesa2020role} also links the negative effects of AI on the environment to the large energy demands of the required data centers. In the influential work \cite{Strubell2019EnergyAP}, authors addressed the environmental toll from state-of-the-art data-intensive natural language processing (NLP) algorithms and expressed it as carbon dioxide emissions resulting from running the algorithms on costly and energy-demanding data centers. Similarly, in \cite{zhou2020hulk} authors present a benchmark for the study of energy efficiency in NLP algorithms; quantifying it in terms of the cost corresponding to the development phases. Other related works define metrics to capture this effect of AI on the environment. For instance, five efficiency metrics are introduced in \cite{Dalgren2019GreenMLA} to assess resource consumption of machine learning algorithms that consider accuracy, model size, time and CPU/GPU energy consumption for the training and inference phases. \cite{garcia2019estimation} presents different approaches at a system level to estimate the energy consumption of a set of state-of-the-art machine learning algorithms. The considered approaches include performance counters (PMC), real-time power estimation, instruction-level estimation, and a hardware-level estimation among others. It is clear that, despite the apparent awareness of part of the research community regarding energy consumption, there is a lack of general effort in the full research community to address those issues.

Recently, there has been an increasing concern in the research community about the negative impact that artificial intelligence and machine learning can have on the environment. For instance, works such as \cite{schwartz2019green}, \cite{vinuesa2020role}, and \cite{Strubell2019EnergyAP} discuss the efficiency of computation-intensive deep learning algorithms\footnote{Consider that the number of computations required by deep learning algorithms has increased more than 300000x in the last decade \cite{schwartz2019green}.} --- such as natural language processing (NLP) --- and express their impact on the environment based on the carbon footprint left by the worldwide data centers used to train and run them. Related works have defined metrics to assess the energy consumption of machine learning algorithms, such as the energy efficiency of their development phases \cite{zhou2020hulk},  the accuracy, model size, time and CPU/GPU energy consumption for the training and inference phases \cite{Dalgren2019GreenMLA}, as well as other system level performance counters (PMC) like real-time, instruction-level, and a hardware-level power estimation \cite{garcia2019estimation}. Despite this rising awareness about energy consumption in AI, actions are yet to be taken to understand the roots of the problem and present potential solutions to alleviate it.


% SUBSECTION ========================================================================================
\subsection{Energy demand in robotics}\label{sec:energy_in_robotics}
%Recent statistics \cite{IFR2019} show a clear upward trend in the worldwide demand for industrial robots. Such trend is directly linked to a significant increase in the consumption of electric energy. Reduction of the energy consumption from industrial robots has been studied in recent works, see \cite{schroder2014, chalmers2015, mohammed2014, chemnitz2011}. Similarly, research has focused on exploiting robots with elastic actuation to make more efficient use of energy \cite{scalera2019natural, carabin2017review, bolivar2017general, haddadin2011optimal,haddadin2012intrinsically}. Moreover, research initiatives have worked on methods to make intelligent manufacturing more efficient, e.g., \cite{aerus2014, bukata2016energy}. 
%
%For instance, a review of various methods and technologies for improving energetic performance of industrial robots was presented in \cite{carabin2017review}. It covers aspects such as the appropriate selection of robot type as well as additions and replacements of hardware, such as for the storage and sharing of energy. Software methods are also discussed, center at enhanced motion planning, via trajectory optimization and operation scheduling. 
%
%
%Albeit these initial approaches, there is a lack of studies that consider the rising number of operational industrial robots and the direct effect that they will have in the electric energy consumption worldwide. There is, however, a relevant related study focused on the robotics market \cite{barnett_2017} in the United States. It projected the future demand for electricity that the ever-increasing introduction of robots in many aspects of human life will originate based on the energy interactions for robot types across various sectors. The study estimates a demand of 22,822 GWh in 2025. Authors highlight, among other aspects, the need of increased R\&D to develop optimized motion patterns to reduce energy consumption while maximizing performance. In spite of the contribution of these works. there is no clear estimate of the energy demand rise that the increasing number of robot installations will bring into the picture nor a strategy to address this problem.

Recent statistics \cite{IFR2019} show a clear upward trend in the worldwide demand for industrial robots. Such trend is directly linked to a significant increase in the consumption of electric energy. For instance, although focused only on the robotics market in the United States, the study in \cite{barnett_2017} projects that 22,822 GWh will be demanded by the ever-increasing introduction of robots in many aspects of human life. Yet, although the reduction of the energy consumption from industrial robots has been studied in a number of works \cite{schroder2014, chalmers2015, mohammed2014, chemnitz2011} providing alternatives to make robots more energy efficient ---e.g. exploiting robots with elastic actuation \cite{scalera2019natural, carabin2017review, bolivar2017general, haddadin2011optimal,haddadin2012intrinsically} or methods for better hardware selection and storage and sharing of energy and optimized motion planning \cite{carabin2017review}---,  there is still no clear estimate of the energy demand rise that the increasing number of robot installations will bring into the picture nor a strategy to address this problem.

%Reduction of the energy consumption from industrial robots has been studied in recent works, see \cite{schroder2014, chalmers2015, mohammed2014, chemnitz2011}. Similarly, alternatives to make robots more energy efficient have been studied, e.g. exploiting robots with elastic actuation \cite{scalera2019natural, carabin2017review, bolivar2017general, haddadin2011optimal,haddadin2012intrinsically} or methods for better hardware selection and storage and sharing of energy and optimized motion planning \cite{carabin2017review}. In spite of the contribution of these works. there is no clear estimate of the energy demand rise that the increasing number of robot installations will bring into the picture nor a strategy to address this problem.


In summary, this section has shown that works in separate research fronts have studied the energy consumption of different aspects of embodied AI. However, a holistic understanding to the energy challenges arising from the advent of embodied AI is still missing. We attempt to contribute to this understanding elaborating on these challenges and positioning the recent paradigm of collective learning \textbf{\textcolor{red}{REF TO SIBLING PAPER}} as the core strategy that can leverage scalability, and knowledge exchange to achieve energy efficiency in embodied AI.